{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ensemble Learning",
   "id": "575c4cb5c554f2f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Imports and Setting up the Kaggle API\n",
    "### Create .env File and Set KAGGLE_KEY and KAGGLE_USERNAME as Kaggle Username and Key in .env File\n",
    "### Example:\n",
    "KAGGLE_KEY=API_KEY\n",
    "KAGGLE_USERNAME=USERNAME\n",
    "\n",
    "load_dotenv will take .env and set key pairs as environmental variables in Python"
   ],
   "id": "297190960c102f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:20.004488Z",
     "start_time": "2025-12-16T21:19:17.614410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import copy\n",
    "\n"
   ],
   "id": "856954414c93e72",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setting the API Instance and downloading dataset",
   "id": "6b090646430f7745"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:20.511344Z",
     "start_time": "2025-12-16T21:19:20.011445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "apiInstance=kaggle.KaggleApi()\n",
    "apiInstance.dataset_download_files('fedesoriano/stroke-prediction-dataset', unzip=True)"
   ],
   "id": "471110ed8d675f2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "Prior to model training, a comprehensive preprocessing pipeline was implemented to ensure data quality, prevent information leakage, and prepare the dataset for use with a boosted tree model (XGBoost). Preprocessing decisions were made with careful consideration of the dataset’s mixed data types, missing values, and significant class imbalance.\n"
   ],
   "id": "11c341ab6eddc815"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:20.534281Z",
     "start_time": "2025-12-16T21:19:20.517550Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strokeData=pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "strokeDataFeatures=strokeData.iloc[:,1:-1]\n",
    "#iloc[rows,columns] we used : on rows as : specifies a range so a range with no upper or lower bound means taking everyting\n",
    "#1:-1 means a range from 1(dropping our first column) to -1(which really means our last column)\n",
    "#dropping the first column our ID column since it has no predictive power and can potentially cause any learners we use to develop patterns on it\n",
    "#dropping the last column since we only want our features and not the labels\n",
    "strokeDataLabels=strokeData.iloc[:,-1]\n",
    "#getting only the last column as we only want the labels"
   ],
   "id": "5b0679b68ba4edbb",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train-Test Split\n",
    "The dataset was first divided into training and test sets using a stratified split to preserve the original class distribution of the stroke outcome variable.\n",
    "\n",
    "- Training set: 80%\n",
    "- Test set: 20%\n",
    "- Stratification applied on the target label to maintain class balance\n",
    "\n",
    "This split was performed before any preprocessing steps to avoid data leakage from the test set into the training process."
   ],
   "id": "3203c29a28c94754"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:20.549139Z",
     "start_time": "2025-12-16T21:19:20.539221Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainFeatures, testFeatures, trainLabels, testLabels = train_test_split(\n",
    "    strokeDataFeatures, strokeDataLabels, test_size=0.2, random_state=42,stratify=strokeDataLabels)\n",
    "#splitting the dataset into test and train data\n",
    "#we want to split before we do any other preprocessing as we don't want our train feature preprocessing learning information from our test features"
   ],
   "id": "e7716f8d9bcbeea3",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Imputation and Encoding\n",
    "To prepare the dataset for training with a boosted tree model, imputation and feature encoding were performed as part of a unified preprocessing step. These procedures were designed to handle missing values, convert categorical variables into numerical representations, and ensure consistency between the training and test datasets while avoiding information leakage.\n"
   ],
   "id": "c8eab62d087d7172"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:20.572020Z",
     "start_time": "2025-12-16T21:19:20.557025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(trainFeatures.info())\n",
    "\n",
    "print(\"\\n Train Data Labels vs Presence of Nans in Label Column \\n\")\n",
    "print(f\"{trainFeatures.isnull().any()}\\n\")\n",
    "\n",
    "print(\"\\n Test Data Labels vs Presence of Nans in Label Column \\n\")\n",
    "print(f\"{testFeatures.isnull().any()}\\n\")\n",
    "#BMI is the only column with NaNs"
   ],
   "id": "c0b9f15663beb258",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4088 entries, 845 to 5052\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   gender             4088 non-null   object \n",
      " 1   age                4088 non-null   float64\n",
      " 2   hypertension       4088 non-null   int64  \n",
      " 3   heart_disease      4088 non-null   int64  \n",
      " 4   ever_married       4088 non-null   object \n",
      " 5   work_type          4088 non-null   object \n",
      " 6   Residence_type     4088 non-null   object \n",
      " 7   avg_glucose_level  4088 non-null   float64\n",
      " 8   bmi                3918 non-null   float64\n",
      " 9   smoking_status     4088 non-null   object \n",
      "dtypes: float64(3), int64(2), object(5)\n",
      "memory usage: 351.3+ KB\n",
      "None\n",
      "\n",
      " Train Data Labels vs Presence of Nans in Label Column \n",
      "\n",
      "gender               False\n",
      "age                  False\n",
      "hypertension         False\n",
      "heart_disease        False\n",
      "ever_married         False\n",
      "work_type            False\n",
      "Residence_type       False\n",
      "avg_glucose_level    False\n",
      "bmi                   True\n",
      "smoking_status       False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      " Test Data Labels vs Presence of Nans in Label Column \n",
      "\n",
      "gender               False\n",
      "age                  False\n",
      "hypertension         False\n",
      "heart_disease        False\n",
      "ever_married         False\n",
      "work_type            False\n",
      "Residence_type       False\n",
      "avg_glucose_level    False\n",
      "bmi                   True\n",
      "smoking_status       False\n",
      "dtype: bool\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Missing Value Imputation\n",
    "\n",
    "The dataset contained missing values in the BMI feature. To address this, a median-based imputation strategy was applied.\n",
    "\n",
    "The imputer was fit exclusively on the training data to learn the median BMI value. This learned value was then used to transform both the training and test datasets. Median imputation was selected because it is robust to outliers and well suited for skewed distributions, particularly in imbalanced medical datasets.\n",
    "\n",
    "### Categorical Feature Identification and Encoding\n",
    "\n",
    "Categorical features were analyzed based on their data type and number of unique categories.\n",
    "\n",
    "Features with more than two unique categorical values were encoded using one-hot encoding. This approach ensures that no artificial ordinal relationships are introduced between categories and allows each category to be represented independently.\n",
    "\n",
    "Binary categorical features were encoded using ordinal encoding, mapping each category directly to a numerical value. This representation is appropriate for binary variables and avoids unnecessary expansion of the feature space.\n",
    "\n",
    "To ensure robustness during transformation of the test set, the one-hot encoder was configured to ignore unseen categories that may appear in new data.\n",
    "\n",
    "### Numerical Feature Scaling\n",
    "\n",
    "Continuous numerical features were standardized using z-score normalization. These features included age, average glucose level, and BMI. Binary numerical features were excluded from scaling, as their values already lie on a consistent scale.\n",
    "\n",
    "The scaling parameters were learned from the training data and then applied to the test data to prevent information leakage.\n",
    "\n",
    "### Column Transformation Pipeline\n",
    "\n",
    "All preprocessing steps were applied using a column transformation framework, which allowed different transformations to be applied in parallel to specific feature subsets. Ordinal encoding, one-hot encoding, and feature scaling were applied to their respective columns, while all remaining features were passed through unchanged."
   ],
   "id": "7ca82692c18c514e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:20.823679Z",
     "start_time": "2025-12-16T21:19:20.588249Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "#we want to use standard scaler to scale the inputs for the numberical inputs to avoid problems with weights for different model types\n",
    "#standard scaler or minmax scaler are good but we will use StandardScaler\n",
    "#we want to use onehotencoder for categorical columns\n",
    "# that have more than 2 possible awnsers, ordinal or label encoder for categorical columsn that have only 2 possible awnsers and then we want to use the columnTransformer to apply these encoders to the columsn we want to apply them to\n",
    "#these lines help us figure out which columsn need to be onehotencoded and which need to be converted to binary\n",
    "\n",
    "trainFeaturesToIterate=copy.copy(trainFeatures)\n",
    "columnsToOneHotEncode=[]\n",
    "print('\\nColumns to be OneHotEncoded: \\n')\n",
    "for column in trainFeaturesToIterate:\n",
    "    if (trainFeatures[column].nunique()>2) & (trainFeatures[column].dtype == 'object'):\n",
    "        columnsToOneHotEncode.append(column)\n",
    "        print(f'{column} has unique categories of {trainFeatures[column].unique()}')\n",
    "        trainFeaturesToIterate.drop(column, axis=1, inplace=True)\n",
    "#gender work_type and smoking_status should be OneHotEncoded to avoid the learners accidentally ranking\n",
    "columnsToOrdinalEncoded=[]\n",
    "print('\\nColumns to be converted to Binary \\n')\n",
    "for column in trainFeaturesToIterate:\n",
    "    if (trainFeatures[column].nunique()==2) & (trainFeatures[column].dtype == 'object'):\n",
    "        columnsToOrdinalEncoded.append(column)\n",
    "        print(f'{column} has unique categories of {trainFeatures[column].unique()}')\n",
    "        trainFeaturesToIterate.drop(column, axis=1, inplace=True)\n",
    "#ever_married and residence_type can be converted to binary 0 and 1 since there are only 2 values\n",
    "\n",
    "\n",
    "columnsToScale=[]\n",
    "print('\\nColumns to be scaled \\n')\n",
    "for column in trainFeaturesToIterate:\n",
    "    if trainFeatures[column].dtype in ['float64', 'int64']:\n",
    "        uniqueVals = set(trainFeatures[column].unique())\n",
    "        if uniqueVals != {0, 1}:\n",
    "            columnsToScale.append(column)\n",
    "            print(f'{column} is a numerical category that must be scaled')\n",
    "            trainFeaturesToIterate.drop(column, axis=1, inplace=True)\n",
    "#age,avg_glucose_level and bmi are our numerical data values which needs to be scalled down\n",
    "\n",
    "#column transformer takes in an array of tuples(each tuples has three values) each tuple is represents an encoder you will use on some columns in the tuples you have three values the first value is some arbitrary name like 'ordinalEncoder' and the second vlaue is the function for the encoder itself like OrdinalEncoder() the third value is a list of the column indices or column names if the data is a dataframe which you want that specific encoder to be used on so in this case we only want our OrdinalEncoder to\n",
    "\n",
    "#columnTransformers runs each transformer in parallel and we want to impute values before we scale them so we want to create a pipeline for our numberical non discrete columns i.e. the columns that need to be scaled\n",
    "\n",
    "#before we do aanything we need to impute the missing values that are present in bmi in both the trainFeatures and the testFeatures\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(trainFeatures[[\"bmi\"]])\n",
    "trainFeatures[\"bmi\"] = imputer.transform(trainFeatures[[\"bmi\"]])\n",
    "testFeatures[\"bmi\"]  = imputer.transform(testFeatures[[\"bmi\"]])\n",
    "\n",
    "ct=ColumnTransformer(transformers=[('ordinalEncoder', OrdinalEncoder(), columnsToOrdinalEncoded),('oneHotEncoder', OneHotEncoder(handle_unknown=\"ignore\"), columnsToOneHotEncode),('scaler', StandardScaler(),columnsToScale)],remainder='passthrough')\n",
    "del trainFeaturesToIterate\n",
    "trainFeatures=ct.fit_transform(trainFeatures)\n",
    "testFeatures=ct.transform(testFeatures)\n",
    "print(ct.get_feature_names_out())\n",
    "print(pd.DataFrame(trainFeatures).info())\n",
    "print(pd.DataFrame(testFeatures).info())"
   ],
   "id": "2120871b07da2b7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns to be OneHotEncoded: \n",
      "\n",
      "gender has unique categories of ['Female' 'Male' 'Other']\n",
      "work_type has unique categories of ['Private' 'Self-employed' 'Govt_job' 'children' 'Never_worked']\n",
      "smoking_status has unique categories of ['never smoked' 'smokes' 'Unknown' 'formerly smoked']\n",
      "\n",
      "Columns to be converted to Binary \n",
      "\n",
      "ever_married has unique categories of ['Yes' 'No']\n",
      "Residence_type has unique categories of ['Urban' 'Rural']\n",
      "\n",
      "Columns to be scaled \n",
      "\n",
      "age is a numerical category that must be scaled\n",
      "avg_glucose_level is a numerical category that must be scaled\n",
      "bmi is a numerical category that must be scaled\n",
      "['ordinalEncoder__ever_married' 'ordinalEncoder__Residence_type'\n",
      " 'oneHotEncoder__gender_Female' 'oneHotEncoder__gender_Male'\n",
      " 'oneHotEncoder__gender_Other' 'oneHotEncoder__work_type_Govt_job'\n",
      " 'oneHotEncoder__work_type_Never_worked'\n",
      " 'oneHotEncoder__work_type_Private'\n",
      " 'oneHotEncoder__work_type_Self-employed'\n",
      " 'oneHotEncoder__work_type_children'\n",
      " 'oneHotEncoder__smoking_status_Unknown'\n",
      " 'oneHotEncoder__smoking_status_formerly smoked'\n",
      " 'oneHotEncoder__smoking_status_never smoked'\n",
      " 'oneHotEncoder__smoking_status_smokes' 'scaler__age'\n",
      " 'scaler__avg_glucose_level' 'scaler__bmi' 'remainder__hypertension'\n",
      " 'remainder__heart_disease']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4088 entries, 0 to 4087\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       4088 non-null   float64\n",
      " 1   1       4088 non-null   float64\n",
      " 2   2       4088 non-null   float64\n",
      " 3   3       4088 non-null   float64\n",
      " 4   4       4088 non-null   float64\n",
      " 5   5       4088 non-null   float64\n",
      " 6   6       4088 non-null   float64\n",
      " 7   7       4088 non-null   float64\n",
      " 8   8       4088 non-null   float64\n",
      " 9   9       4088 non-null   float64\n",
      " 10  10      4088 non-null   float64\n",
      " 11  11      4088 non-null   float64\n",
      " 12  12      4088 non-null   float64\n",
      " 13  13      4088 non-null   float64\n",
      " 14  14      4088 non-null   float64\n",
      " 15  15      4088 non-null   float64\n",
      " 16  16      4088 non-null   float64\n",
      " 17  17      4088 non-null   float64\n",
      " 18  18      4088 non-null   float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 606.9 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1022 entries, 0 to 1021\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       1022 non-null   float64\n",
      " 1   1       1022 non-null   float64\n",
      " 2   2       1022 non-null   float64\n",
      " 3   3       1022 non-null   float64\n",
      " 4   4       1022 non-null   float64\n",
      " 5   5       1022 non-null   float64\n",
      " 6   6       1022 non-null   float64\n",
      " 7   7       1022 non-null   float64\n",
      " 8   8       1022 non-null   float64\n",
      " 9   9       1022 non-null   float64\n",
      " 10  10      1022 non-null   float64\n",
      " 11  11      1022 non-null   float64\n",
      " 12  12      1022 non-null   float64\n",
      " 13  13      1022 non-null   float64\n",
      " 14  14      1022 non-null   float64\n",
      " 15  15      1022 non-null   float64\n",
      " 16  16      1022 non-null   float64\n",
      " 17  17      1022 non-null   float64\n",
      " 18  18      1022 non-null   float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 151.8 KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:20.834813Z",
     "start_time": "2025-12-16T21:19:20.830317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(trainFeatures[0])\n",
    "print(trainLabels[0])"
   ],
   "id": "6974528e3f2575d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.          1.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  1.          0.          0.20566087 -0.8199733   0.543113    0.\n",
      "  0.        ]\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Creation, Setup and Analytical Measurement\n",
   "id": "d88e7d519c7caf4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## XGBoost Testing\n",
    "To establish a baseline understanding of XGBoost behavior on the stroke prediction dataset, an initial model was trained using default-style hyperparameters and the root mean squared error (RMSE) evaluation metric. Training and evaluation performance were monitored across boosting iterations using both the training and test datasets.\n",
    "\n",
    "During this initial testing phase, the model exhibited clear signs of overfitting. As the number of boosting rounds increased, the training error steadily decreased while the evaluation error increased. This divergence indicates that the model was increasingly fitting noise in the training data rather than learning generalizable patterns\n",
    "\n",
    "In addition to overfitting concerns, RMSE was found to be an unsuitable evaluation metric for this task. Stroke prediction is a binary classification problem with a highly imbalanced class distribution, where the minority class represents stroke occurrences. RMSE does not explicitly account for class imbalance and does not differentiate between false positives and false negatives. As a result, a model optimized for RMSE may achieve low error by primarily predicting the majority class, while failing to correctly identify stroke cases.\n",
    "\n",
    "Because false negatives are particularly costly in medical screening contexts, evaluation metrics that emphasize sensitivity to the positive class are more appropriate. This observation motivated a transition away from RMSE toward a classification-focused metric that better reflects model performance on the minority class.\n",
    "\n",
    "Based on these findings, subsequent experiments replaced RMSE with the area under the precision–recall curve (PR-AUC) as the primary evaluation metric. PR-AUC provides a more informative assessment of performance for imbalanced classification tasks by directly capturing the trade-off between precision and recall, particularly for rare positive outcomes such as strokes."
   ],
   "id": "5322d87e0d4dfdff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:21.405646Z",
     "start_time": "2025-12-16T21:19:20.848239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dTrain=xgb.DMatrix(trainFeatures, trainLabels)\n",
    "dTest = xgb.DMatrix(testFeatures, testLabels)\n",
    "#xgboost parameters we will need to optimize to make ours ensemble training better\n",
    "\n",
    "#Tree Booster Parameters\n",
    "\n",
    "#eta is our learning rate\n",
    "#gamma is the minimum loss reduction to make a further leaf partition increasing makes the model less complex\n",
    "#max_depth is max depth of the tree increasing max depth can make the model more complex\n",
    "#min_child_weight relates to the minimum amount of samples can be in a leaf node increasing decrease the model complexity\n",
    "#max_delta_step acts to reduce how fast leaf values can change important for us as the skewed dataset can mean that the tree can change drastically to fit our very low amount of strokes in dataset so by increasing max_delta_step above 0 we can reduce the rate of change of leaf values meaning there is a less change of our skewed dataset from overfitting quickly\n",
    "#subsample tells how much of the dataset to randomly sample before growin tree decreasing number will help reduce overfitting\n",
    "#sampling_method how the dataset samples are chosen uniform means each instance has an equal change of being chosen and gradient based samples are chosen based on which samples have the greatest gradient, most variation between predicted label and actual label\n",
    "#colsample_bylevel,colsample_bytree,colsample_bynode control the amount of features(columns) can be used at each by object so colsample_bylevel=0.5 means that only 50% of the available features can be used to split at the level decreasing features used can reduce overfitting by forcing the tree to use different features\n",
    "#lambda alpha are L1,L2 regularization terms respectively increasing them increases how conservative the model is\n",
    "#tree_method controls tree construction algorithm exact approx hist\n",
    "#scale_pos_weight controls the balance of positive and negative weights\n",
    "#updater A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees.\n",
    "#process_type type of boosting process to run we don't need to change for us\n",
    "#grow_policy controls how new nodes are added to the tree\n",
    "#max_leaves controls maxiumum number of nodes to add\n",
    "\n",
    "\n",
    "#Learning Task Parameters\n",
    "\n",
    "#objective controls which objective function we use\n",
    "#base_score controls initial prediction of all instances\n",
    "#eval_metric evaluation metrics for validation data\n",
    "#seed random number seed\n",
    "\n",
    "\n",
    "params = {'objective':'binary:logistic','eval_metric':'rmse','eta':0.3,'gamma':0,'max_depth':6,'min_child_weight':1,'max_delta_step':0,'subsample':1,'sampling_method':'uniform','colsample_bylevel':1,'colsample_bytree':1,'colsample_bynode':1,'alpha':0,'lambda':1,'tree_method':'auto','scale_pos_weight':1,'refresh_leaf':1,'process_type':'default','grow_policy':'depthwise','max_leaves':0,'max_bin':256,}\n",
    "\n",
    "evalList = [(dTrain, 'train'), (dTest, 'eval')]\n",
    "\n",
    "\"\"\"params (Dict[str, Any]) – Booster params.\n",
    "\n",
    "dtrain (DMatrix) – Data to be trained.\n",
    "\n",
    "num_boost_round (int) – Number of boosting iterations.\n",
    "\n",
    "evals (Sequence[Tuple[DMatrix, str]] | None) – List of validation sets for which metrics will evaluated during training. Validation metrics will help us track the performance of the model.\"\"\"\n",
    "\n",
    "bst=xgb.train(params=params, dtrain=dTrain, num_boost_round=100, evals=evalList, verbose_eval=True)\n",
    "\n"
   ],
   "id": "c1d42be88109ae29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.20464\teval-rmse:0.20994\n",
      "[1]\ttrain-rmse:0.19721\teval-rmse:0.20856\n",
      "[2]\ttrain-rmse:0.19265\teval-rmse:0.20738\n",
      "[3]\ttrain-rmse:0.18952\teval-rmse:0.20752\n",
      "[4]\ttrain-rmse:0.18695\teval-rmse:0.20711\n",
      "[5]\ttrain-rmse:0.18448\teval-rmse:0.20630\n",
      "[6]\ttrain-rmse:0.18127\teval-rmse:0.20668\n",
      "[7]\ttrain-rmse:0.17991\teval-rmse:0.20652\n",
      "[8]\ttrain-rmse:0.17783\teval-rmse:0.20647\n",
      "[9]\ttrain-rmse:0.17699\teval-rmse:0.20659\n",
      "[10]\ttrain-rmse:0.17572\teval-rmse:0.20706\n",
      "[11]\ttrain-rmse:0.17315\teval-rmse:0.20746\n",
      "[12]\ttrain-rmse:0.17271\teval-rmse:0.20709\n",
      "[13]\ttrain-rmse:0.17230\teval-rmse:0.20718\n",
      "[14]\ttrain-rmse:0.17142\teval-rmse:0.20731\n",
      "[15]\ttrain-rmse:0.17073\teval-rmse:0.20733\n",
      "[16]\ttrain-rmse:0.17028\teval-rmse:0.20737\n",
      "[17]\ttrain-rmse:0.16938\teval-rmse:0.20750\n",
      "[18]\ttrain-rmse:0.16787\teval-rmse:0.20777\n",
      "[19]\ttrain-rmse:0.16675\teval-rmse:0.20834\n",
      "[20]\ttrain-rmse:0.16541\teval-rmse:0.20876\n",
      "[21]\ttrain-rmse:0.16435\teval-rmse:0.20845\n",
      "[22]\ttrain-rmse:0.16227\teval-rmse:0.20919\n",
      "[23]\ttrain-rmse:0.16027\teval-rmse:0.20933\n",
      "[24]\ttrain-rmse:0.15951\teval-rmse:0.20988\n",
      "[25]\ttrain-rmse:0.15695\teval-rmse:0.20991\n",
      "[26]\ttrain-rmse:0.15641\teval-rmse:0.21002\n",
      "[27]\ttrain-rmse:0.15498\teval-rmse:0.21023\n",
      "[28]\ttrain-rmse:0.15440\teval-rmse:0.21020\n",
      "[29]\ttrain-rmse:0.15269\teval-rmse:0.21016\n",
      "[30]\ttrain-rmse:0.15207\teval-rmse:0.21061\n",
      "[31]\ttrain-rmse:0.15141\teval-rmse:0.21060\n",
      "[32]\ttrain-rmse:0.15023\teval-rmse:0.21088\n",
      "[33]\ttrain-rmse:0.14979\teval-rmse:0.21076\n",
      "[34]\ttrain-rmse:0.14860\teval-rmse:0.21044\n",
      "[35]\ttrain-rmse:0.14723\teval-rmse:0.21047\n",
      "[36]\ttrain-rmse:0.14698\teval-rmse:0.21044\n",
      "[37]\ttrain-rmse:0.14645\teval-rmse:0.21093\n",
      "[38]\ttrain-rmse:0.14581\teval-rmse:0.21116\n",
      "[39]\ttrain-rmse:0.14377\teval-rmse:0.21126\n",
      "[40]\ttrain-rmse:0.14348\teval-rmse:0.21133\n",
      "[41]\ttrain-rmse:0.14077\teval-rmse:0.21106\n",
      "[42]\ttrain-rmse:0.13818\teval-rmse:0.21179\n",
      "[43]\ttrain-rmse:0.13710\teval-rmse:0.21179\n",
      "[44]\ttrain-rmse:0.13551\teval-rmse:0.21232\n",
      "[45]\ttrain-rmse:0.13183\teval-rmse:0.21272\n",
      "[46]\ttrain-rmse:0.12886\teval-rmse:0.21308\n",
      "[47]\ttrain-rmse:0.12808\teval-rmse:0.21310\n",
      "[48]\ttrain-rmse:0.12577\teval-rmse:0.21346\n",
      "[49]\ttrain-rmse:0.12522\teval-rmse:0.21346\n",
      "[50]\ttrain-rmse:0.12338\teval-rmse:0.21470\n",
      "[51]\ttrain-rmse:0.12225\teval-rmse:0.21514\n",
      "[52]\ttrain-rmse:0.12110\teval-rmse:0.21489\n",
      "[53]\ttrain-rmse:0.12036\teval-rmse:0.21475\n",
      "[54]\ttrain-rmse:0.11855\teval-rmse:0.21513\n",
      "[55]\ttrain-rmse:0.11806\teval-rmse:0.21505\n",
      "[56]\ttrain-rmse:0.11692\teval-rmse:0.21504\n",
      "[57]\ttrain-rmse:0.11601\teval-rmse:0.21506\n",
      "[58]\ttrain-rmse:0.11542\teval-rmse:0.21496\n",
      "[59]\ttrain-rmse:0.11535\teval-rmse:0.21496\n",
      "[60]\ttrain-rmse:0.11385\teval-rmse:0.21478\n",
      "[61]\ttrain-rmse:0.11279\teval-rmse:0.21500\n",
      "[62]\ttrain-rmse:0.11039\teval-rmse:0.21531\n",
      "[63]\ttrain-rmse:0.10779\teval-rmse:0.21595\n",
      "[64]\ttrain-rmse:0.10667\teval-rmse:0.21637\n",
      "[65]\ttrain-rmse:0.10660\teval-rmse:0.21634\n",
      "[66]\ttrain-rmse:0.10496\teval-rmse:0.21647\n",
      "[67]\ttrain-rmse:0.10478\teval-rmse:0.21641\n",
      "[68]\ttrain-rmse:0.10304\teval-rmse:0.21659\n",
      "[69]\ttrain-rmse:0.10233\teval-rmse:0.21664\n",
      "[70]\ttrain-rmse:0.10193\teval-rmse:0.21647\n",
      "[71]\ttrain-rmse:0.10015\teval-rmse:0.21659\n",
      "[72]\ttrain-rmse:0.09802\teval-rmse:0.21676\n",
      "[73]\ttrain-rmse:0.09755\teval-rmse:0.21671\n",
      "[74]\ttrain-rmse:0.09714\teval-rmse:0.21675\n",
      "[75]\ttrain-rmse:0.09539\teval-rmse:0.21694\n",
      "[76]\ttrain-rmse:0.09508\teval-rmse:0.21733\n",
      "[77]\ttrain-rmse:0.09452\teval-rmse:0.21725\n",
      "[78]\ttrain-rmse:0.09373\teval-rmse:0.21725\n",
      "[79]\ttrain-rmse:0.09237\teval-rmse:0.21755\n",
      "[80]\ttrain-rmse:0.09091\teval-rmse:0.21762\n",
      "[81]\ttrain-rmse:0.08988\teval-rmse:0.21775\n",
      "[82]\ttrain-rmse:0.08902\teval-rmse:0.21806\n",
      "[83]\ttrain-rmse:0.08791\teval-rmse:0.21809\n",
      "[84]\ttrain-rmse:0.08661\teval-rmse:0.21862\n",
      "[85]\ttrain-rmse:0.08592\teval-rmse:0.21854\n",
      "[86]\ttrain-rmse:0.08577\teval-rmse:0.21871\n",
      "[87]\ttrain-rmse:0.08528\teval-rmse:0.21905\n",
      "[88]\ttrain-rmse:0.08436\teval-rmse:0.21933\n",
      "[89]\ttrain-rmse:0.08381\teval-rmse:0.21921\n",
      "[90]\ttrain-rmse:0.08298\teval-rmse:0.21950\n",
      "[91]\ttrain-rmse:0.08248\teval-rmse:0.21948\n",
      "[92]\ttrain-rmse:0.08188\teval-rmse:0.21940\n",
      "[93]\ttrain-rmse:0.08177\teval-rmse:0.21945\n",
      "[94]\ttrain-rmse:0.08141\teval-rmse:0.21950\n",
      "[95]\ttrain-rmse:0.08116\teval-rmse:0.21971\n",
      "[96]\ttrain-rmse:0.08102\teval-rmse:0.21998\n",
      "[97]\ttrain-rmse:0.08083\teval-rmse:0.22014\n",
      "[98]\ttrain-rmse:0.08024\teval-rmse:0.22011\n",
      "[99]\ttrain-rmse:0.08014\teval-rmse:0.22025\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Clear overfitting occurring as our train rmse goes down but our eval rmse goes up.",
   "id": "1155d4ee7e42860b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:21.861472Z",
     "start_time": "2025-12-16T21:19:21.412695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params['eval_metric']='aucpr'\n",
    "#auc is a good metric for classification which is the problem we are trying to solve and it is sensitive to false negatives i.e. missing a stroke which we do not want occuring\n",
    "bst1=xgb.train(params=params, dtrain=dTrain, num_boost_round=100, evals=evalList, verbose_eval=True)"
   ],
   "id": "b102a2ec270322f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-aucpr:0.28578\teval-aucpr:0.20355\n",
      "[1]\ttrain-aucpr:0.40758\teval-aucpr:0.18801\n",
      "[2]\ttrain-aucpr:0.44578\teval-aucpr:0.19553\n",
      "[3]\ttrain-aucpr:0.46944\teval-aucpr:0.19111\n",
      "[4]\ttrain-aucpr:0.49687\teval-aucpr:0.20160\n",
      "[5]\ttrain-aucpr:0.52073\teval-aucpr:0.22373\n",
      "[6]\ttrain-aucpr:0.55634\teval-aucpr:0.21643\n",
      "[7]\ttrain-aucpr:0.57205\teval-aucpr:0.21640\n",
      "[8]\ttrain-aucpr:0.59467\teval-aucpr:0.22401\n",
      "[9]\ttrain-aucpr:0.60335\teval-aucpr:0.21595\n",
      "[10]\ttrain-aucpr:0.61445\teval-aucpr:0.21053\n",
      "[11]\ttrain-aucpr:0.63836\teval-aucpr:0.20160\n",
      "[12]\ttrain-aucpr:0.64307\teval-aucpr:0.20710\n",
      "[13]\ttrain-aucpr:0.64541\teval-aucpr:0.20583\n",
      "[14]\ttrain-aucpr:0.65381\teval-aucpr:0.20348\n",
      "[15]\ttrain-aucpr:0.66102\teval-aucpr:0.20541\n",
      "[16]\ttrain-aucpr:0.66666\teval-aucpr:0.20442\n",
      "[17]\ttrain-aucpr:0.67090\teval-aucpr:0.20488\n",
      "[18]\ttrain-aucpr:0.68740\teval-aucpr:0.20506\n",
      "[19]\ttrain-aucpr:0.69941\teval-aucpr:0.20039\n",
      "[20]\ttrain-aucpr:0.71265\teval-aucpr:0.19849\n",
      "[21]\ttrain-aucpr:0.72171\teval-aucpr:0.19972\n",
      "[22]\ttrain-aucpr:0.73568\teval-aucpr:0.19443\n",
      "[23]\ttrain-aucpr:0.75324\teval-aucpr:0.19478\n",
      "[24]\ttrain-aucpr:0.75857\teval-aucpr:0.19135\n",
      "[25]\ttrain-aucpr:0.78213\teval-aucpr:0.19232\n",
      "[26]\ttrain-aucpr:0.78635\teval-aucpr:0.18993\n",
      "[27]\ttrain-aucpr:0.79701\teval-aucpr:0.18772\n",
      "[28]\ttrain-aucpr:0.79983\teval-aucpr:0.18903\n",
      "[29]\ttrain-aucpr:0.81326\teval-aucpr:0.20376\n",
      "[30]\ttrain-aucpr:0.81643\teval-aucpr:0.20236\n",
      "[31]\ttrain-aucpr:0.81973\teval-aucpr:0.20221\n",
      "[32]\ttrain-aucpr:0.82706\teval-aucpr:0.20196\n",
      "[33]\ttrain-aucpr:0.82987\teval-aucpr:0.20327\n",
      "[34]\ttrain-aucpr:0.83817\teval-aucpr:0.20976\n",
      "[35]\ttrain-aucpr:0.84740\teval-aucpr:0.20739\n",
      "[36]\ttrain-aucpr:0.84895\teval-aucpr:0.20844\n",
      "[37]\ttrain-aucpr:0.85400\teval-aucpr:0.20062\n",
      "[38]\ttrain-aucpr:0.85557\teval-aucpr:0.19960\n",
      "[39]\ttrain-aucpr:0.86831\teval-aucpr:0.19956\n",
      "[40]\ttrain-aucpr:0.87049\teval-aucpr:0.19921\n",
      "[41]\ttrain-aucpr:0.88800\teval-aucpr:0.18023\n",
      "[42]\ttrain-aucpr:0.90013\teval-aucpr:0.17915\n",
      "[43]\ttrain-aucpr:0.90553\teval-aucpr:0.19198\n",
      "[44]\ttrain-aucpr:0.91134\teval-aucpr:0.17756\n",
      "[45]\ttrain-aucpr:0.92637\teval-aucpr:0.19688\n",
      "[46]\ttrain-aucpr:0.93736\teval-aucpr:0.20072\n",
      "[47]\ttrain-aucpr:0.93969\teval-aucpr:0.19481\n",
      "[48]\ttrain-aucpr:0.94840\teval-aucpr:0.18741\n",
      "[49]\ttrain-aucpr:0.94972\teval-aucpr:0.18822\n",
      "[50]\ttrain-aucpr:0.95558\teval-aucpr:0.17833\n",
      "[51]\ttrain-aucpr:0.95766\teval-aucpr:0.17502\n",
      "[52]\ttrain-aucpr:0.96032\teval-aucpr:0.17626\n",
      "[53]\ttrain-aucpr:0.96136\teval-aucpr:0.17824\n",
      "[54]\ttrain-aucpr:0.96614\teval-aucpr:0.16332\n",
      "[55]\ttrain-aucpr:0.96653\teval-aucpr:0.17923\n",
      "[56]\ttrain-aucpr:0.96944\teval-aucpr:0.17888\n",
      "[57]\ttrain-aucpr:0.97164\teval-aucpr:0.17786\n",
      "[58]\ttrain-aucpr:0.97168\teval-aucpr:0.17983\n",
      "[59]\ttrain-aucpr:0.97199\teval-aucpr:0.18033\n",
      "[60]\ttrain-aucpr:0.97458\teval-aucpr:0.18007\n",
      "[61]\ttrain-aucpr:0.97602\teval-aucpr:0.17558\n",
      "[62]\ttrain-aucpr:0.98001\teval-aucpr:0.17531\n",
      "[63]\ttrain-aucpr:0.98345\teval-aucpr:0.17677\n",
      "[64]\ttrain-aucpr:0.98503\teval-aucpr:0.17311\n",
      "[65]\ttrain-aucpr:0.98518\teval-aucpr:0.17313\n",
      "[66]\ttrain-aucpr:0.98683\teval-aucpr:0.17300\n",
      "[67]\ttrain-aucpr:0.98696\teval-aucpr:0.17396\n",
      "[68]\ttrain-aucpr:0.98941\teval-aucpr:0.17143\n",
      "[69]\ttrain-aucpr:0.98977\teval-aucpr:0.17114\n",
      "[70]\ttrain-aucpr:0.98989\teval-aucpr:0.17172\n",
      "[71]\ttrain-aucpr:0.99148\teval-aucpr:0.17230\n",
      "[72]\ttrain-aucpr:0.99247\teval-aucpr:0.17276\n",
      "[73]\ttrain-aucpr:0.99270\teval-aucpr:0.17233\n",
      "[74]\ttrain-aucpr:0.99290\teval-aucpr:0.17003\n",
      "[75]\ttrain-aucpr:0.99387\teval-aucpr:0.17198\n",
      "[76]\ttrain-aucpr:0.99383\teval-aucpr:0.17000\n",
      "[77]\ttrain-aucpr:0.99396\teval-aucpr:0.17010\n",
      "[78]\ttrain-aucpr:0.99495\teval-aucpr:0.16854\n",
      "[79]\ttrain-aucpr:0.99513\teval-aucpr:0.16825\n",
      "[80]\ttrain-aucpr:0.99612\teval-aucpr:0.16797\n",
      "[81]\ttrain-aucpr:0.99693\teval-aucpr:0.16988\n",
      "[82]\ttrain-aucpr:0.99719\teval-aucpr:0.16704\n",
      "[83]\ttrain-aucpr:0.99745\teval-aucpr:0.16582\n",
      "[84]\ttrain-aucpr:0.99799\teval-aucpr:0.16487\n",
      "[85]\ttrain-aucpr:0.99809\teval-aucpr:0.16274\n",
      "[86]\ttrain-aucpr:0.99816\teval-aucpr:0.16262\n",
      "[87]\ttrain-aucpr:0.99821\teval-aucpr:0.16200\n",
      "[88]\ttrain-aucpr:0.99838\teval-aucpr:0.16259\n",
      "[89]\ttrain-aucpr:0.99855\teval-aucpr:0.14775\n",
      "[90]\ttrain-aucpr:0.99891\teval-aucpr:0.14688\n",
      "[91]\ttrain-aucpr:0.99893\teval-aucpr:0.14735\n",
      "[92]\ttrain-aucpr:0.99906\teval-aucpr:0.14693\n",
      "[93]\ttrain-aucpr:0.99908\teval-aucpr:0.14710\n",
      "[94]\ttrain-aucpr:0.99906\teval-aucpr:0.16060\n",
      "[95]\ttrain-aucpr:0.99906\teval-aucpr:0.15884\n",
      "[96]\ttrain-aucpr:0.99906\teval-aucpr:0.15902\n",
      "[97]\ttrain-aucpr:0.99906\teval-aucpr:0.15812\n",
      "[98]\ttrain-aucpr:0.99922\teval-aucpr:0.15900\n",
      "[99]\ttrain-aucpr:0.99922\teval-aucpr:0.15868\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Defining Metrics",
   "id": "5a716bcf8c318c36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To evaluate the computational efficiency and environmental impact of the boosted tree model (XGBoost), a set of model-appropriate metrics was defined. While many efficiency and sustainability metrics apply broadly across machine learning models, certain neural-network-specific measures require adaptation when applied to tree-based algorithms. In order to enable direct comparison with neural network models, floating point operations (FLOPs) were estimated for the boosted tree model using a tree-based approximation. The metrics below were selected and adapted to accurately reflect the computational characteristics of boosted decision trees.\n",
    "\n",
    "### Computational Metrics\n",
    "\n",
    "The following metrics were used to quantify computational efficiency for the boosted tree model:\n",
    "\n",
    "- Training time\n",
    "- Average CPU utilization\n",
    "- Peak CPU utilization\n",
    "- RAM usage\n",
    "- Floating point operations (FLOPs)\n",
    "\n",
    "#### Computational Metric Formulas\n",
    "\n",
    "- Training time (s)\n",
    "  = End time − Start time\n",
    "\n",
    "- Average CPU Utilization (%)\n",
    "  = (Sum of all CPU utilization samples) / (Number of samples)\n",
    "\n",
    "- Peak CPU Utilization (%)\n",
    "  = Maximum value observed across all CPU samples\n",
    "\n",
    "- RAM Usage (GB)\n",
    "  = Resident Set Size (bytes) / 1,000,000,000\n",
    "\n",
    "- FLOPs per sample\n",
    "  = Number of trees × Average tree depth\n",
    "\n",
    "- Total FLOPs\n",
    "  = FLOPs per sample × Number of samples × C\n",
    "\n",
    "  where C is a constant factor accounting for additional operations during training, including gradient computation and tree updates.\n",
    "\n",
    "##### Notes on RAM\n",
    "- Resident Set Size represents the total memory consumed by the training process, including the dataset stored in XGBoost’s DMatrix format and the learned tree structures.\n",
    "- Conversion from bytes to gigabytes (GB) allows for consistent comparison across models.\n",
    "\n",
    "##### Notes on FLOPs\n",
    "- Unlike neural networks, boosted trees do not perform dense matrix multiplications.\n",
    "- FLOPs were estimated by counting decision node evaluations during tree traversal, with each node comparison approximated as a single floating point operation.\n",
    "- Average tree depth serves as a proxy for the number of node evaluations per tree.\n",
    "- While this FLOP estimate is an approximation, it provides a consistent and interpretable measure of computational cost that enables comparison with neural network models.\n",
    "\n",
    "### Environmental Metrics\n",
    "\n",
    "The following metrics were used to estimate the environmental impact of training the boosted tree model:\n",
    "\n",
    "- Energy consumption\n",
    "- CO2 emissions\n",
    "- Water consumption\n",
    "\n",
    "#### Environmental Metric Formulas\n",
    "\n",
    "- Energy Consumption (Wh)\n",
    "  = (Power used during training × Training time) / 3600\n",
    "\n",
    "- CO2 Emissions (g)\n",
    "  = Energy consumption × Carbon intensity\n",
    "\n",
    "- Water Consumption (mL)\n",
    "  = Energy consumption × Local water intensity\n",
    "\n",
    "##### Notes on Energy Consumption\n",
    "- Training was performed on a personal computing device using CPU-based training.\n",
    "- A constant power draw was assumed for simplicity.\n",
    "- Total power consumption reflects CPU usage during training which is gotten through sudo powermetrics --samplers cpu_power with an average value of 3.6W\n",
    "- Division by 3600 converts seconds to hours.\n",
    "\n",
    "##### Notes on CO2 Emissions\n",
    "- Carbon intensity was assumed to be 0.408 gCO2/Wh, corresponding to the regional electricity grid average.\n",
    "- Power Usage Effectiveness (PUE) was not included, as the model was trained on a personal laptop rather than a data center.\n",
    "\n",
    "##### Notes on Water Consumption\n",
    "- A water intensity of 2.18 L/kWh (2.18 mL/Wh) was used based on U.S. average estimates.\n",
    "- Region-specific water intensity data was unavailable, so a national average was applied.\n",
    "- Cooling and infrastructure overhead factors were omitted for simplicity.\n",
    "\n",
    "These metrics provide a comprehensive evaluation of both the computational cost and environmental footprint of training the boosted tree model while enabling direct comparison with neural network based approaches."
   ],
   "id": "271beabe79ee18c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:21.877146Z",
     "start_time": "2025-12-16T21:19:21.866954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time, psutil, os, threading\n",
    "import numpy as np\n",
    "\n",
    "class TreeMetrics:\n",
    "    def __init__(self, name=\"XGBoost\", carbonIntensity=0.408, waterIntensity=2.18):\n",
    "        self.name = name\n",
    "        self.carbonIntensity = carbonIntensity\n",
    "        self.waterIntensity = waterIntensity\n",
    "        self.process = psutil.Process(os.getpid())\n",
    "        self.cpu_samples = []\n",
    "        self.total_flops = 0\n",
    "\n",
    "    # -------------------------\n",
    "    # Runtime monitoring\n",
    "    # -------------------------\n",
    "    def start(self):\n",
    "        self.startTime = time.time()\n",
    "        self.cpu_samples = []\n",
    "        self.monitoring = True\n",
    "        self.monitor_thread = threading.Thread(\n",
    "            target=self._monitor_cpu, daemon=True\n",
    "        )\n",
    "        self.monitor_thread.start()\n",
    "        print(f\"Tracking: {self.name}\")\n",
    "\n",
    "    def _monitor_cpu(self):\n",
    "        while self.monitoring:\n",
    "            self.cpu_samples.append(psutil.cpu_percent(interval=0.1))\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    def stop(self):\n",
    "        self.endTime = time.time()\n",
    "        self.monitoring = False\n",
    "        if hasattr(self, \"monitor_thread\"):\n",
    "            self.monitor_thread.join(timeout=2.0)\n",
    "\n",
    "    # -------------------------\n",
    "    # FLOPs accounting\n",
    "    # -------------------------\n",
    "    def add_flops_training(\n",
    "        self,\n",
    "        num_samples,\n",
    "        num_boost_rounds,\n",
    "        avg_tree_depth,\n",
    "        k_node=4,\n",
    "        C_train=3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        FLOPs for XGBoost training:\n",
    "        num_samples × num_trees × depth × node_ops × train_ops\n",
    "        \"\"\"\n",
    "        flops = (\n",
    "            num_samples *\n",
    "            num_boost_rounds *\n",
    "            avg_tree_depth *\n",
    "            k_node *\n",
    "            C_train\n",
    "        )\n",
    "        self.total_flops += flops\n",
    "\n",
    "    def add_flops_gridsearch(\n",
    "        self,\n",
    "        paramGrid,\n",
    "        n_estimators,\n",
    "        cv,\n",
    "        num_samples,\n",
    "        k_node=4,\n",
    "        C_train=3\n",
    "    ):\n",
    "        num_param_combos = 1\n",
    "        for v in paramGrid.values():\n",
    "            num_param_combos *= len(v)\n",
    "\n",
    "        avg_depth = np.mean(paramGrid[\"max_depth\"])\n",
    "\n",
    "        flops_per_model = (\n",
    "            num_samples *\n",
    "            n_estimators *\n",
    "            avg_depth *\n",
    "            k_node *\n",
    "            C_train\n",
    "        )\n",
    "\n",
    "        total = flops_per_model * num_param_combos * cv\n",
    "        self.total_flops += total\n",
    "\n",
    "    def compute_final_model_flops(\n",
    "        self,\n",
    "        num_samples,\n",
    "        num_boost_rounds,\n",
    "        avg_tree_depth,\n",
    "        k_node=4,\n",
    "        C_train=3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Computes FLOPs for training the final selected XGBoost model only.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_samples : int\n",
    "            Number of training samples\n",
    "        num_boost_rounds : int\n",
    "            Number of trees actually trained (best_iteration + 1)\n",
    "        avg_tree_depth : int\n",
    "            Maximum / average depth of trees\n",
    "        k_node : int\n",
    "            FLOPs per node evaluation (comparison + branching)\n",
    "        C_train : int\n",
    "            Training multiplier (forward + gradient + update)\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        flops : int\n",
    "            Estimated total FLOPs for final model training\n",
    "        \"\"\"\n",
    "\n",
    "        flops = (\n",
    "            num_samples *\n",
    "            num_boost_rounds *\n",
    "            avg_tree_depth *\n",
    "            k_node *\n",
    "            C_train\n",
    "        )\n",
    "\n",
    "        return flops\n",
    "    # -------------------------\n",
    "    # Final report\n",
    "    # -------------------------\n",
    "    def report(self, powerUsage=3.6):\n",
    "        trainingTime = self.endTime - self.startTime\n",
    "\n",
    "        avg_cpu = (\n",
    "            sum(self.cpu_samples) / len(self.cpu_samples)\n",
    "            if self.cpu_samples else 0\n",
    "        )\n",
    "        peak_cpu = max(self.cpu_samples) if self.cpu_samples else 0\n",
    "\n",
    "        energy = powerUsage * trainingTime / 3600\n",
    "        co2 = energy * self.carbonIntensity\n",
    "        water = energy * self.waterIntensity\n",
    "\n",
    "        print(\"\\n===== FINAL COMPUTE REPORT =====\")\n",
    "        print(f\"Training time: {trainingTime:.2f}s ({trainingTime/60:.2f} min)\")\n",
    "        print(f\"Average CPU Utilization: {avg_cpu:.2f}%\")\n",
    "        print(f\"Peak CPU Utilization: {peak_cpu:.2f}%\")\n",
    "        print(f\"RAM Usage: {self.process.memory_info().rss/1e9:.2f} GB\")\n",
    "        print(f\"Total Estimated FLOPs: {self.total_flops/1e9:.3f} GFLOPs\")\n",
    "        print(f\"Energy Consumption: {energy:.6f} Wh\")\n",
    "        print(f\"CO₂ Emissions: {co2:.6f} g\")\n",
    "        print(f\"Water Consumption: {water:.4f} mL\")\n"
   ],
   "id": "b816eefbe6d6bf2c",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Creation and Tuning",
   "id": "a6d0801b9d39c528"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Changing the eval metric to AUC gives us a better view of the issue and our overfitting problem. To solve there are a couple things we can consider\n",
    "\n",
    "early stopping when the evalulation auc does not go significantly up after a certain number of runs to stop overfitting early\n",
    "\n",
    "changing our other hyperparameters to avoid overfitting\n",
    "\n",
    "While xgboost library does not have a grid search feature to help optimize parameter finding, sklearn does and xgboost has a sklearn wrapper to allow us to use both xgboost and the sklearn functions"
   ],
   "id": "3404252732367b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:19:21.888415Z",
     "start_time": "2025-12-16T21:19:21.884039Z"
    }
   },
   "cell_type": "code",
   "source": [
    "metrics = TreeMetrics(name=\"XGBoost Final Model\")\n",
    "metrics.start()\n",
    "\"\"\"scale_pos_weights Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances)\"\"\"\n",
    "neg = (trainLabels == 0).sum()\n",
    "pos = (trainLabels == 1).sum()\n",
    "\n",
    "scale_pos_weight = neg / pos\n",
    "print(scale_pos_weight)\n"
   ],
   "id": "68e01c993dacaa96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking: XGBoost Final Model\n",
      "19.542713567839197\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have alot of parameters to check so we want to get rid of some them, learning rate and n_estimators are going to be linked together so we want to change those after we find out the best combination of other hyperparameters\n",
    "\n",
    "For the rest of the hyperparameters we can set some of these and not use grid search for them such as subsample, colsample_bytree, colsample_bylevel and colsample_bynode to help overfitting we can set it to 0.7 along with colsample_bytree we can just set that to 0.7 and min_child_weight can be set to 3\n",
    "\n",
    "Since our dataset is relatively small compared to other data sets we can use an exact tree method rather than a hist allowing us to get rid of some parameters like grow_policy and max_leaves"
   ],
   "id": "ba3b500035afe04e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:25:57.317125Z",
     "start_time": "2025-12-16T21:19:21.899943Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "bstSklearnWrapper=xgb.XGBClassifier(n_estimators=100, max_depth=6,tree_method='exact',learning_rate=0.1,objective=\"binary:logistic\", booster='gbtree',gamma=0,min_child_weight=3,max_delta_step=1,subsample=0.7,colsample_bytree=0.7,colsample_bylevel=1,colsample_bynode=1,reg_alpha=0,reg_lambda=1,scale_pos_weight=scale_pos_weight,random_state=42,eval_metric='aucpr')\n",
    "paramGrid={'max_depth':[3,4,5,6],'max_delta_step':[1,2,3,4],'min_child_weight':[2,3,4],'gamma':[0,1],'reg_alpha':[0,1],\"reg_lambda\":[0,1]}\n",
    "grid=GridSearchCV(estimator=bstSklearnWrapper,param_grid=paramGrid,cv=5,scoring='average_precision',verbose=1)\n",
    "grid.fit(trainFeatures, trainLabels)\n",
    "metrics.add_flops_gridsearch(\n",
    "    paramGrid=paramGrid,\n",
    "    n_estimators=bstSklearnWrapper.n_estimators,\n",
    "    cv=grid.cv,\n",
    "    num_samples=trainFeatures.shape[0]\n",
    ")\n",
    "\n"
   ],
   "id": "4a74b8ff56414148",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We have found good parameters however we want to now play around with our learning rate and the number of estimators to do this effectively and not waste time we want to be able to use early stopping which we can not do in a GridSearchCV so we are going to use another way to play around with these 2",
   "id": "d4eef98465a622c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:25:57.343234Z",
     "start_time": "2025-12-16T21:25:57.338365Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bestModel=grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "skParams = grid.best_estimator_.get_params()\n",
    "print(skParams)\n",
    "baseParams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"aucpr\",\n",
    "    \"tree_method\": skParams[\"tree_method\"],\n",
    "    \"booster\": skParams[\"booster\"],\n",
    "    \"max_depth\": skParams[\"max_depth\"],\n",
    "    \"min_child_weight\": skParams[\"min_child_weight\"],\n",
    "    \"max_delta_step\": skParams[\"max_delta_step\"],\n",
    "    \"subsample\": skParams[\"subsample\"],\n",
    "    \"colsample_bytree\": skParams[\"colsample_bytree\"],\n",
    "    \"gamma\": skParams[\"gamma\"],\n",
    "    \"alpha\": skParams[\"reg_alpha\"],\n",
    "    \"lambda\": skParams[\"reg_lambda\"],\n",
    "    \"scale_pos_weight\": skParams[\"scale_pos_weight\"],\n",
    "    \"seed\": skParams[\"random_state\"],\n",
    "}\n",
    "print(baseParams)\n",
    "\n"
   ],
   "id": "54dc4d29874d421e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0, 'max_delta_step': 3, 'max_depth': 5, 'min_child_weight': 3, 'reg_alpha': 1, 'reg_lambda': 0}\n",
      "0.23254648276381293\n",
      "{'objective': 'binary:logistic', 'base_score': None, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 0.7, 'device': None, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': 'aucpr', 'feature_types': None, 'feature_weights': None, 'gamma': 0, 'grow_policy': None, 'importance_type': None, 'interaction_constraints': None, 'learning_rate': 0.1, 'max_bin': None, 'max_cat_threshold': None, 'max_cat_to_onehot': None, 'max_delta_step': 3, 'max_depth': 5, 'max_leaves': None, 'min_child_weight': 3, 'missing': nan, 'monotone_constraints': None, 'multi_strategy': None, 'n_estimators': 100, 'n_jobs': None, 'num_parallel_tree': None, 'random_state': 42, 'reg_alpha': 1, 'reg_lambda': 0, 'sampling_method': None, 'scale_pos_weight': np.float64(19.542713567839197), 'subsample': 0.7, 'tree_method': 'exact', 'validate_parameters': None, 'verbosity': None}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "To find the best learning rate and the number of estimators needed for the best model we want to loop through early stopping rounds values and then loop through learning rates allowing the model to grow using the best parameters that we found through the grid search and using early stopping to find the best number of estimators for each learning rate. Once we have these models we want to compare the precision of each one nd choose the on with the highest pr_auc. To find the best early stopping rounds,",
   "id": "82c393db86c95830"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:25:57.381566Z",
     "start_time": "2025-12-16T21:25:57.373197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#splitting the train data into train data and validation data\n",
    "trainFeatures, valFeatures, trainLabels, valLabels = train_test_split(\n",
    "    trainFeatures, trainLabels, test_size=0.2, random_state=42,stratify=trainLabels)\n",
    "dVal = xgb.DMatrix(valFeatures, valLabels)\n",
    "dTrain = xgb.DMatrix(trainFeatures, trainLabels)"
   ],
   "id": "17f3a3a98e26b9a7",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:26:14.154162Z",
     "start_time": "2025-12-16T21:25:57.390396Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "learningRates = [0.001,0.003,0.005,0.008,0.01, 0.03, 0.05, 0.08, 0.1,0.3]\n",
    "earlyStoppingRounds = [20,30,40,50,60,70,80,100]\n",
    "\n",
    "results = []\n",
    "for learningRate in learningRates:\n",
    "    for patience in earlyStoppingRounds:\n",
    "        params = baseParams.copy()\n",
    "        params['learning_rate'] = learningRate\n",
    "        print(params)\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dTrain,\n",
    "            num_boost_round=2000,\n",
    "            evals=[(dVal, \"val\")],\n",
    "            early_stopping_rounds=patience,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        metrics.add_flops_training(\n",
    "            num_samples=dTrain.num_row(),\n",
    "            num_boost_rounds=booster.best_iteration + 1,\n",
    "            avg_tree_depth=baseParams[\"max_depth\"]\n",
    "        )\n",
    "\n",
    "        yValProba = booster.predict(dVal)\n",
    "        pr_auc = average_precision_score(valLabels, yValProba)\n",
    "\n",
    "        results.append({\n",
    "            \"learningRate\": learningRate,\n",
    "            \"earlyStoppingRounds\": patience,\n",
    "            \"bestNumEstimators\": booster.best_iteration + 1,\n",
    "            \"valPrAuc\": pr_auc,\n",
    "            \"model\": booster\n",
    "        })\n",
    "bestResult = max(results, key=lambda x: x[\"valPrAuc\"])\n",
    "print(bestResult)\n",
    "bestModel=bestResult[\"model\"]\n",
    "bestParamsFinal = baseParams.copy()\n",
    "bestParamsFinal[\"learning_rate\"] = bestResult[\"learningRate\"]"
   ],
   "id": "70ade23e8abaaea3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.001}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.001}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.001}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.001}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.001}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.001}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.001}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.001}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.003}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.003}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.003}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.003}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.003}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.003}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.003}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.003}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.005}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.005}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.005}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.005}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.005}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.005}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.005}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.005}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.008}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.008}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.008}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.008}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.008}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.008}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.008}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.008}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.01}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.01}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.01}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.01}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.01}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.01}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.01}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.01}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.03}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.03}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.03}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.03}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.03}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.03}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.03}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.03}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.05}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.05}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.05}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.05}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.05}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.05}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.05}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.05}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.08}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.08}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.08}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.08}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.08}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.08}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.08}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.08}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.1}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.1}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.1}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.1}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.1}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.1}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.1}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.1}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.3}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.3}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.3}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.3}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.3}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.3}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.3}\n",
      "{'objective': 'binary:logistic', 'eval_metric': 'aucpr', 'tree_method': 'exact', 'booster': 'gbtree', 'max_depth': 5, 'min_child_weight': 3, 'max_delta_step': 3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'gamma': 0, 'alpha': 1, 'lambda': 0, 'scale_pos_weight': np.float64(19.542713567839197), 'seed': 42, 'learning_rate': 0.3}\n",
      "{'learningRate': 0.008, 'earlyStoppingRounds': 50, 'bestNumEstimators': 26, 'valPrAuc': 0.18387008596980342, 'model': <xgboost.core.Booster object at 0x125020eb0>}\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The last thing we want to optimize is our threshold, or our probability value above which we label the data a 1. There are a couple ways to choose the threshold we could go by best precision, best recall, best f1, and additionally, the way we end up going, by filtering the results and only taking the results that have a recall above a certain number which for us we want to be 0.85 for our validation case since our dataset are possible stroke cases we want to have a high recall to make sure strokes dont slip through.",
   "id": "ecaa40f4726c6073"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:26:14.756829Z",
     "start_time": "2025-12-16T21:26:14.163567Z"
    }
   },
   "cell_type": "code",
   "source": [
    "yValProba = bestModel.predict(dVal)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "thresholdResults = []\n",
    "\n",
    "for t in thresholds:\n",
    "    yValPred = (yValProba >= t).astype(int)\n",
    "\n",
    "    precision = precision_score(valLabels, yValPred, zero_division=0)\n",
    "    recall    = recall_score(valLabels, yValPred, zero_division=0)\n",
    "    f1        = f1_score(valLabels, yValPred, zero_division=0)\n",
    "\n",
    "    thresholdResults.append({\n",
    "        \"threshold\": t,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    })\n",
    "recallTarget=0.85\n",
    "\n",
    "thresholdDf = pd.DataFrame(thresholdResults)\n",
    "filtered = thresholdDf[thresholdDf[\"recall\"] >= recallTarget]\n",
    "bestRow = filtered.loc[filtered[\"precision\"].idxmax()]\n",
    "bestThreshold = bestRow[\"threshold\"]\n",
    "\n",
    "print(\"Best threshold (max precision):\")\n",
    "print(bestRow)\n",
    "metrics.stop()\n"
   ],
   "id": "5c74be7113bc8bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold (max precision):\n",
      "threshold    0.470000\n",
      "precision    0.143460\n",
      "recall       0.850000\n",
      "f1           0.245487\n",
      "Name: 46, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T21:29:14.701178Z",
     "start_time": "2025-12-16T21:29:14.684980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "yTestProba = bestModel.predict(dTest)\n",
    "yTestPred  = (yTestProba >= bestThreshold).astype(int)\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, classification_report\n",
    "\n",
    "print(\"Test PR-AUC:\", average_precision_score(testLabels, yTestProba))\n",
    "print(\"Test ROC-AUC:\", roc_auc_score(testLabels, yTestProba))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(testLabels, yTestPred, digits=4))\n",
    "\n",
    "metrics.report()\n",
    "final_model_flops = metrics.compute_final_model_flops(\n",
    "    num_samples=dTrain.num_row(),\n",
    "    num_boost_rounds=bestModel.best_iteration + 1,\n",
    "    avg_tree_depth=bestParamsFinal[\"max_depth\"]\n",
    ")\n",
    "\n",
    "print(\"\\n===== FINAL MODEL ONLY =====\")\n",
    "print(f\"Final Model FLOPs: {final_model_flops/1e9:.3f} GFLOPs\")\n",
    "print(f\"Final Model Share of Total GFLOPs: {(final_model_flops / metrics.total_flops) * 100:.2f}%\")\n",
    "\n"
   ],
   "id": "2fa595ca823fb201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PR-AUC: 0.2513923525978292\n",
      "Test ROC-AUC: 0.8470781893004115\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9864    0.7469    0.8501       972\n",
      "           1     0.1399    0.8000    0.2381        50\n",
      "\n",
      "    accuracy                         0.7495      1022\n",
      "   macro avg     0.5631    0.7735    0.5441      1022\n",
      "weighted avg     0.9450    0.7495    0.8202      1022\n",
      "\n",
      "\n",
      "===== FINAL COMPUTE REPORT =====\n",
      "Training time: 412.62s (6.88 min)\n",
      "Average CPU Utilization: 47.03%\n",
      "Peak CPU Utilization: 98.90%\n",
      "RAM Usage: 0.09 GB\n",
      "Total Estimated FLOPs: 42.739 GFLOPs\n",
      "Energy Consumption: 0.412619 Wh\n",
      "CO₂ Emissions: 0.168349 g\n",
      "Water Consumption: 0.8995 mL\n",
      "\n",
      "===== FINAL MODEL ONLY =====\n",
      "Final Model FLOPs: 0.005 GFLOPs\n",
      "Final Model Share of Total GFLOPs: 0.01%\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "d4560d0c10dde725"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
