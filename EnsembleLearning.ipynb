{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ensemble Learning",
   "id": "575c4cb5c554f2f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Imports and Setting up the Kaggle API\n",
    "### Create .env File and Set KAGGLE_KEY and KAGGLE_USERNAME as Kaggle Username and Key in .env File\n",
    "### Example:\n",
    "KAGGLE_KEY=API_KEY\n",
    "KAGGLE_USERNAME=USERNAME\n",
    "\n",
    "load_dotenv will take .env and set key pairs as environmental variables in Python"
   ],
   "id": "297190960c102f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:54:57.722647Z",
     "start_time": "2025-12-16T23:54:57.712998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "import copy\n",
    "\n"
   ],
   "id": "856954414c93e72",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setting the API Instance and downloading dataset",
   "id": "6b090646430f7745"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:54:58.113587Z",
     "start_time": "2025-12-16T23:54:57.729392Z"
    }
   },
   "cell_type": "code",
   "source": [
    "apiInstance=kaggle.KaggleApi()\n",
    "apiInstance.dataset_download_files('fedesoriano/stroke-prediction-dataset', unzip=True)"
   ],
   "id": "471110ed8d675f2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocessing\n",
    "Prior to model training, a comprehensive preprocessing pipeline was implemented to ensure data quality, prevent information leakage, and prepare the dataset for use with a boosted tree model (XGBoost). Preprocessing decisions were made with careful consideration of the dataset’s mixed data types, missing values, and significant class imbalance.\n"
   ],
   "id": "11c341ab6eddc815"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:54:58.135297Z",
     "start_time": "2025-12-16T23:54:58.118175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strokeData=pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "strokeDataFeatures=strokeData.iloc[:,1:-1]\n",
    "#iloc[rows,columns] we used : on rows as : specifies a range so a range with no upper or lower bound means taking everyting\n",
    "#1:-1 means a range from 1(dropping our first column) to -1(which really means our last column)\n",
    "#dropping the first column our ID column since it has no predictive power and can potentially cause any learners we use to develop patterns on it\n",
    "#dropping the last column since we only want our features and not the labels\n",
    "strokeDataLabels=strokeData.iloc[:,-1]\n",
    "#getting only the last column as we only want the labels"
   ],
   "id": "5b0679b68ba4edbb",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Train-Test Split\n",
    "The dataset was first divided into training and test sets using a stratified split to preserve the original class distribution of the stroke outcome variable.\n",
    "\n",
    "- Training set: 80%\n",
    "- Test set: 20%\n",
    "- Stratification applied on the target label to maintain class balance\n",
    "\n",
    "This split was performed before any preprocessing steps to avoid data leakage from the test set into the training process."
   ],
   "id": "3203c29a28c94754"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:54:58.147404Z",
     "start_time": "2025-12-16T23:54:58.140061Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainFeatures, testFeatures, trainLabels, testLabels = train_test_split(\n",
    "    strokeDataFeatures, strokeDataLabels, test_size=0.2, random_state=42,stratify=strokeDataLabels)\n",
    "#splitting the dataset into test and train data\n",
    "#we want to split before we do any other preprocessing as we don't want our train feature preprocessing learning information from our test features"
   ],
   "id": "e7716f8d9bcbeea3",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Imputation and Encoding\n",
    "To prepare the dataset for training with a boosted tree model, imputation and feature encoding were performed as part of a unified preprocessing step. These procedures were designed to handle missing values, convert categorical variables into numerical representations, and ensure consistency between the training and test datasets while avoiding information leakage.\n"
   ],
   "id": "c8eab62d087d7172"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:54:58.164361Z",
     "start_time": "2025-12-16T23:54:58.152769Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(trainFeatures.info())\n",
    "\n",
    "print(\"\\n Train Data Labels vs Presence of Nans in Label Column \\n\")\n",
    "print(f\"{trainFeatures.isnull().any()}\\n\")\n",
    "\n",
    "print(\"\\n Test Data Labels vs Presence of Nans in Label Column \\n\")\n",
    "print(f\"{testFeatures.isnull().any()}\\n\")\n",
    "#BMI is the only column with NaNs"
   ],
   "id": "c0b9f15663beb258",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 4088 entries, 845 to 5052\n",
      "Data columns (total 10 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   gender             4088 non-null   object \n",
      " 1   age                4088 non-null   float64\n",
      " 2   hypertension       4088 non-null   int64  \n",
      " 3   heart_disease      4088 non-null   int64  \n",
      " 4   ever_married       4088 non-null   object \n",
      " 5   work_type          4088 non-null   object \n",
      " 6   Residence_type     4088 non-null   object \n",
      " 7   avg_glucose_level  4088 non-null   float64\n",
      " 8   bmi                3918 non-null   float64\n",
      " 9   smoking_status     4088 non-null   object \n",
      "dtypes: float64(3), int64(2), object(5)\n",
      "memory usage: 351.3+ KB\n",
      "None\n",
      "\n",
      " Train Data Labels vs Presence of Nans in Label Column \n",
      "\n",
      "gender               False\n",
      "age                  False\n",
      "hypertension         False\n",
      "heart_disease        False\n",
      "ever_married         False\n",
      "work_type            False\n",
      "Residence_type       False\n",
      "avg_glucose_level    False\n",
      "bmi                   True\n",
      "smoking_status       False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      " Test Data Labels vs Presence of Nans in Label Column \n",
      "\n",
      "gender               False\n",
      "age                  False\n",
      "hypertension         False\n",
      "heart_disease        False\n",
      "ever_married         False\n",
      "work_type            False\n",
      "Residence_type       False\n",
      "avg_glucose_level    False\n",
      "bmi                   True\n",
      "smoking_status       False\n",
      "dtype: bool\n",
      "\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Missing Value Imputation\n",
    "\n",
    "The dataset contained missing values in the BMI feature. To address this, a median-based imputation strategy was applied.\n",
    "\n",
    "The imputer was fit exclusively on the training data to learn the median BMI value. This learned value was then used to transform both the training and test datasets. Median imputation was selected because it is robust to outliers and well suited for skewed distributions, particularly in imbalanced medical datasets.\n",
    "\n",
    "### Categorical Feature Identification and Encoding\n",
    "\n",
    "Categorical features were analyzed based on their data type and number of unique categories.\n",
    "\n",
    "Features with more than two unique categorical values were encoded using one-hot encoding. This approach ensures that no artificial ordinal relationships are introduced between categories and allows each category to be represented independently.\n",
    "\n",
    "Binary categorical features were encoded using ordinal encoding, mapping each category directly to a numerical value. This representation is appropriate for binary variables and avoids unnecessary expansion of the feature space.\n",
    "\n",
    "To ensure robustness during transformation of the test set, the one-hot encoder was configured to ignore unseen categories that may appear in new data.\n",
    "\n",
    "### Numerical Feature Scaling\n",
    "\n",
    "Continuous numerical features were standardized using z-score normalization. These features included age, average glucose level, and BMI. Binary numerical features were excluded from scaling, as their values already lie on a consistent scale.\n",
    "\n",
    "The scaling parameters were learned from the training data and then applied to the test data to prevent information leakage.\n",
    "\n",
    "### Column Transformation Pipeline\n",
    "\n",
    "All preprocessing steps were applied using a column transformation framework, which allowed different transformations to be applied in parallel to specific feature subsets. Ordinal encoding, one-hot encoding, and feature scaling were applied to their respective columns, while all remaining features were passed through unchanged."
   ],
   "id": "7ca82692c18c514e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:54:58.222277Z",
     "start_time": "2025-12-16T23:54:58.173982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "\n",
    "#we want to use standard scaler to scale the inputs for the numberical inputs to avoid problems with weights for different model types\n",
    "#standard scaler or minmax scaler are good but we will use StandardScaler\n",
    "#we want to use onehotencoder for categorical columns\n",
    "# that have more than 2 possible awnsers, ordinal or label encoder for categorical columsn that have only 2 possible awnsers and then we want to use the columnTransformer to apply these encoders to the columsn we want to apply them to\n",
    "#these lines help us figure out which columsn need to be onehotencoded and which need to be converted to binary\n",
    "\n",
    "trainFeaturesToIterate=copy.copy(trainFeatures)\n",
    "columnsToOneHotEncode=[]\n",
    "print('\\nColumns to be OneHotEncoded: \\n')\n",
    "for column in trainFeaturesToIterate:\n",
    "    if (trainFeatures[column].nunique()>2) & (trainFeatures[column].dtype == 'object'):\n",
    "        columnsToOneHotEncode.append(column)\n",
    "        print(f'{column} has unique categories of {trainFeatures[column].unique()}')\n",
    "        trainFeaturesToIterate.drop(column, axis=1, inplace=True)\n",
    "#gender work_type and smoking_status should be OneHotEncoded to avoid the learners accidentally ranking\n",
    "columnsToOrdinalEncoded=[]\n",
    "print('\\nColumns to be converted to Binary \\n')\n",
    "for column in trainFeaturesToIterate:\n",
    "    if (trainFeatures[column].nunique()==2) & (trainFeatures[column].dtype == 'object'):\n",
    "        columnsToOrdinalEncoded.append(column)\n",
    "        print(f'{column} has unique categories of {trainFeatures[column].unique()}')\n",
    "        trainFeaturesToIterate.drop(column, axis=1, inplace=True)\n",
    "#ever_married and residence_type can be converted to binary 0 and 1 since there are only 2 values\n",
    "\n",
    "\n",
    "columnsToScale=[]\n",
    "print('\\nColumns to be scaled \\n')\n",
    "for column in trainFeaturesToIterate:\n",
    "    if trainFeatures[column].dtype in ['float64', 'int64']:\n",
    "        uniqueVals = set(trainFeatures[column].unique())\n",
    "        if uniqueVals != {0, 1}:\n",
    "            columnsToScale.append(column)\n",
    "            print(f'{column} is a numerical category that must be scaled')\n",
    "            trainFeaturesToIterate.drop(column, axis=1, inplace=True)\n",
    "#age,avg_glucose_level and bmi are our numerical data values which needs to be scalled down\n",
    "\n",
    "#column transformer takes in an array of tuples(each tuples has three values) each tuple is represents an encoder you will use on some columns in the tuples you have three values the first value is some arbitrary name like 'ordinalEncoder' and the second vlaue is the function for the encoder itself like OrdinalEncoder() the third value is a list of the column indices or column names if the data is a dataframe which you want that specific encoder to be used on so in this case we only want our OrdinalEncoder to\n",
    "\n",
    "#columnTransformers runs each transformer in parallel and we want to impute values before we scale them so we want to create a pipeline for our numberical non discrete columns i.e. the columns that need to be scaled\n",
    "\n",
    "#before we do aanything we need to impute the missing values that are present in bmi in both the trainFeatures and the testFeatures\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "imputer.fit(trainFeatures[[\"bmi\"]])\n",
    "trainFeatures[\"bmi\"] = imputer.transform(trainFeatures[[\"bmi\"]])\n",
    "testFeatures[\"bmi\"]  = imputer.transform(testFeatures[[\"bmi\"]])\n",
    "\n",
    "ct=ColumnTransformer(transformers=[('ordinalEncoder', OrdinalEncoder(), columnsToOrdinalEncoded),('oneHotEncoder', OneHotEncoder(handle_unknown=\"ignore\"), columnsToOneHotEncode),('scaler', StandardScaler(),columnsToScale)],remainder='passthrough')\n",
    "del trainFeaturesToIterate\n",
    "trainFeatures=ct.fit_transform(trainFeatures)\n",
    "testFeatures=ct.transform(testFeatures)\n",
    "print(ct.get_feature_names_out())\n",
    "print(pd.DataFrame(trainFeatures).info())\n",
    "print(pd.DataFrame(testFeatures).info())"
   ],
   "id": "2120871b07da2b7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns to be OneHotEncoded: \n",
      "\n",
      "gender has unique categories of ['Female' 'Male' 'Other']\n",
      "work_type has unique categories of ['Private' 'Self-employed' 'Govt_job' 'children' 'Never_worked']\n",
      "smoking_status has unique categories of ['never smoked' 'smokes' 'Unknown' 'formerly smoked']\n",
      "\n",
      "Columns to be converted to Binary \n",
      "\n",
      "ever_married has unique categories of ['Yes' 'No']\n",
      "Residence_type has unique categories of ['Urban' 'Rural']\n",
      "\n",
      "Columns to be scaled \n",
      "\n",
      "age is a numerical category that must be scaled\n",
      "avg_glucose_level is a numerical category that must be scaled\n",
      "bmi is a numerical category that must be scaled\n",
      "['ordinalEncoder__ever_married' 'ordinalEncoder__Residence_type'\n",
      " 'oneHotEncoder__gender_Female' 'oneHotEncoder__gender_Male'\n",
      " 'oneHotEncoder__gender_Other' 'oneHotEncoder__work_type_Govt_job'\n",
      " 'oneHotEncoder__work_type_Never_worked'\n",
      " 'oneHotEncoder__work_type_Private'\n",
      " 'oneHotEncoder__work_type_Self-employed'\n",
      " 'oneHotEncoder__work_type_children'\n",
      " 'oneHotEncoder__smoking_status_Unknown'\n",
      " 'oneHotEncoder__smoking_status_formerly smoked'\n",
      " 'oneHotEncoder__smoking_status_never smoked'\n",
      " 'oneHotEncoder__smoking_status_smokes' 'scaler__age'\n",
      " 'scaler__avg_glucose_level' 'scaler__bmi' 'remainder__hypertension'\n",
      " 'remainder__heart_disease']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4088 entries, 0 to 4087\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       4088 non-null   float64\n",
      " 1   1       4088 non-null   float64\n",
      " 2   2       4088 non-null   float64\n",
      " 3   3       4088 non-null   float64\n",
      " 4   4       4088 non-null   float64\n",
      " 5   5       4088 non-null   float64\n",
      " 6   6       4088 non-null   float64\n",
      " 7   7       4088 non-null   float64\n",
      " 8   8       4088 non-null   float64\n",
      " 9   9       4088 non-null   float64\n",
      " 10  10      4088 non-null   float64\n",
      " 11  11      4088 non-null   float64\n",
      " 12  12      4088 non-null   float64\n",
      " 13  13      4088 non-null   float64\n",
      " 14  14      4088 non-null   float64\n",
      " 15  15      4088 non-null   float64\n",
      " 16  16      4088 non-null   float64\n",
      " 17  17      4088 non-null   float64\n",
      " 18  18      4088 non-null   float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 606.9 KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1022 entries, 0 to 1021\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       1022 non-null   float64\n",
      " 1   1       1022 non-null   float64\n",
      " 2   2       1022 non-null   float64\n",
      " 3   3       1022 non-null   float64\n",
      " 4   4       1022 non-null   float64\n",
      " 5   5       1022 non-null   float64\n",
      " 6   6       1022 non-null   float64\n",
      " 7   7       1022 non-null   float64\n",
      " 8   8       1022 non-null   float64\n",
      " 9   9       1022 non-null   float64\n",
      " 10  10      1022 non-null   float64\n",
      " 11  11      1022 non-null   float64\n",
      " 12  12      1022 non-null   float64\n",
      " 13  13      1022 non-null   float64\n",
      " 14  14      1022 non-null   float64\n",
      " 15  15      1022 non-null   float64\n",
      " 16  16      1022 non-null   float64\n",
      " 17  17      1022 non-null   float64\n",
      " 18  18      1022 non-null   float64\n",
      "dtypes: float64(19)\n",
      "memory usage: 151.8 KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:54:58.232286Z",
     "start_time": "2025-12-16T23:54:58.229006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(trainFeatures[0])\n",
    "print(trainLabels[0])"
   ],
   "id": "6974528e3f2575d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.          1.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  1.          0.          0.20566087 -0.8199733   0.543113    0.\n",
      "  0.        ]\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Model Creation, Setup and Analytical Measurement\n",
   "id": "d88e7d519c7caf4a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## XGBoost Testing\n",
    "To establish a baseline understanding of XGBoost behavior on the stroke prediction dataset, an initial model was trained using default-style hyperparameters and the root mean squared error (RMSE) evaluation metric. Training and evaluation performance were monitored across boosting iterations using both the training and test datasets.\n",
    "\n",
    "During this initial testing phase, the model exhibited clear signs of overfitting. As the number of boosting rounds increased, the training error steadily decreased while the evaluation error increased. This divergence indicates that the model was increasingly fitting noise in the training data rather than learning generalizable patterns\n",
    "\n",
    "In addition to overfitting concerns, RMSE was found to be an unsuitable evaluation metric for this task. Stroke prediction is a binary classification problem with a highly imbalanced class distribution, where the minority class represents stroke occurrences. RMSE does not explicitly account for class imbalance and does not differentiate between false positives and false negatives. As a result, a model optimized for RMSE may achieve low error by primarily predicting the majority class, while failing to correctly identify stroke cases.\n",
    "\n",
    "Because false negatives are particularly costly in medical screening contexts, evaluation metrics that emphasize sensitivity to the positive class are more appropriate. This observation motivated a transition away from RMSE toward a classification-focused metric that better reflects model performance on the minority class.\n",
    "\n",
    "Based on these findings, subsequent experiments replaced RMSE with the area under the precision–recall curve (PR-AUC) as the primary evaluation metric. PR-AUC provides a more informative assessment of performance for imbalanced classification tasks by directly capturing the trade-off between precision and recall, particularly for rare positive outcomes such as strokes."
   ],
   "id": "5322d87e0d4dfdff"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:54:58.665714Z",
     "start_time": "2025-12-16T23:54:58.267457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dTrain=xgb.DMatrix(trainFeatures, trainLabels)\n",
    "dTest = xgb.DMatrix(testFeatures, testLabels)\n",
    "#xgboost parameters we will need to optimize to make ours ensemble training better\n",
    "\n",
    "#Tree Booster Parameters\n",
    "\n",
    "#eta is our learning rate\n",
    "#gamma is the minimum loss reduction to make a further leaf partition increasing makes the model less complex\n",
    "#max_depth is max depth of the tree increasing max depth can make the model more complex\n",
    "#min_child_weight relates to the minimum amount of samples can be in a leaf node increasing decrease the model complexity\n",
    "#max_delta_step acts to reduce how fast leaf values can change important for us as the skewed dataset can mean that the tree can change drastically to fit our very low amount of strokes in dataset so by increasing max_delta_step above 0 we can reduce the rate of change of leaf values meaning there is a less change of our skewed dataset from overfitting quickly\n",
    "#subsample tells how much of the dataset to randomly sample before growin tree decreasing number will help reduce overfitting\n",
    "#sampling_method how the dataset samples are chosen uniform means each instance has an equal change of being chosen and gradient based samples are chosen based on which samples have the greatest gradient, most variation between predicted label and actual label\n",
    "#colsample_bylevel,colsample_bytree,colsample_bynode control the amount of features(columns) can be used at each by object so colsample_bylevel=0.5 means that only 50% of the available features can be used to split at the level decreasing features used can reduce overfitting by forcing the tree to use different features\n",
    "#lambda alpha are L1,L2 regularization terms respectively increasing them increases how conservative the model is\n",
    "#tree_method controls tree construction algorithm exact approx hist\n",
    "#scale_pos_weight controls the balance of positive and negative weights\n",
    "#updater A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees.\n",
    "#process_type type of boosting process to run we don't need to change for us\n",
    "#grow_policy controls how new nodes are added to the tree\n",
    "#max_leaves controls maxiumum number of nodes to add\n",
    "\n",
    "\n",
    "#Learning Task Parameters\n",
    "\n",
    "#objective controls which objective function we use\n",
    "#base_score controls initial prediction of all instances\n",
    "#eval_metric evaluation metrics for validation data\n",
    "#seed random number seed\n",
    "\n",
    "\n",
    "params = {'objective':'binary:logistic','eval_metric':'rmse','eta':0.3,'gamma':0,'max_depth':6,'min_child_weight':1,'max_delta_step':0,'subsample':1,'sampling_method':'uniform','colsample_bylevel':1,'colsample_bytree':1,'colsample_bynode':1,'alpha':0,'lambda':1,'tree_method':'auto','scale_pos_weight':1,'refresh_leaf':1,'process_type':'default','grow_policy':'depthwise','max_leaves':0,'max_bin':256,}\n",
    "\n",
    "evalList = [(dTrain, 'train'), (dTest, 'eval')]\n",
    "\n",
    "\"\"\"params (Dict[str, Any]) – Booster params.\n",
    "\n",
    "dtrain (DMatrix) – Data to be trained.\n",
    "\n",
    "num_boost_round (int) – Number of boosting iterations.\n",
    "\n",
    "evals (Sequence[Tuple[DMatrix, str]] | None) – List of validation sets for which metrics will evaluated during training. Validation metrics will help us track the performance of the model.\"\"\"\n",
    "\n",
    "bst=xgb.train(params=params, dtrain=dTrain, num_boost_round=100, evals=evalList, verbose_eval=True)\n",
    "\n"
   ],
   "id": "c1d42be88109ae29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.20464\teval-rmse:0.20994\n",
      "[1]\ttrain-rmse:0.19721\teval-rmse:0.20856\n",
      "[2]\ttrain-rmse:0.19265\teval-rmse:0.20738\n",
      "[3]\ttrain-rmse:0.18952\teval-rmse:0.20752\n",
      "[4]\ttrain-rmse:0.18695\teval-rmse:0.20711\n",
      "[5]\ttrain-rmse:0.18448\teval-rmse:0.20630\n",
      "[6]\ttrain-rmse:0.18127\teval-rmse:0.20668\n",
      "[7]\ttrain-rmse:0.17991\teval-rmse:0.20652\n",
      "[8]\ttrain-rmse:0.17783\teval-rmse:0.20647\n",
      "[9]\ttrain-rmse:0.17699\teval-rmse:0.20659\n",
      "[10]\ttrain-rmse:0.17572\teval-rmse:0.20706\n",
      "[11]\ttrain-rmse:0.17315\teval-rmse:0.20746\n",
      "[12]\ttrain-rmse:0.17271\teval-rmse:0.20709\n",
      "[13]\ttrain-rmse:0.17230\teval-rmse:0.20718\n",
      "[14]\ttrain-rmse:0.17142\teval-rmse:0.20731\n",
      "[15]\ttrain-rmse:0.17073\teval-rmse:0.20733\n",
      "[16]\ttrain-rmse:0.17028\teval-rmse:0.20737\n",
      "[17]\ttrain-rmse:0.16938\teval-rmse:0.20750\n",
      "[18]\ttrain-rmse:0.16787\teval-rmse:0.20777\n",
      "[19]\ttrain-rmse:0.16675\teval-rmse:0.20834\n",
      "[20]\ttrain-rmse:0.16541\teval-rmse:0.20876\n",
      "[21]\ttrain-rmse:0.16435\teval-rmse:0.20845\n",
      "[22]\ttrain-rmse:0.16227\teval-rmse:0.20919\n",
      "[23]\ttrain-rmse:0.16027\teval-rmse:0.20933\n",
      "[24]\ttrain-rmse:0.15951\teval-rmse:0.20988\n",
      "[25]\ttrain-rmse:0.15695\teval-rmse:0.20991\n",
      "[26]\ttrain-rmse:0.15641\teval-rmse:0.21002\n",
      "[27]\ttrain-rmse:0.15498\teval-rmse:0.21023\n",
      "[28]\ttrain-rmse:0.15440\teval-rmse:0.21020\n",
      "[29]\ttrain-rmse:0.15269\teval-rmse:0.21016\n",
      "[30]\ttrain-rmse:0.15207\teval-rmse:0.21061\n",
      "[31]\ttrain-rmse:0.15141\teval-rmse:0.21060\n",
      "[32]\ttrain-rmse:0.15023\teval-rmse:0.21088\n",
      "[33]\ttrain-rmse:0.14979\teval-rmse:0.21076\n",
      "[34]\ttrain-rmse:0.14860\teval-rmse:0.21044\n",
      "[35]\ttrain-rmse:0.14723\teval-rmse:0.21047\n",
      "[36]\ttrain-rmse:0.14698\teval-rmse:0.21044\n",
      "[37]\ttrain-rmse:0.14645\teval-rmse:0.21093\n",
      "[38]\ttrain-rmse:0.14581\teval-rmse:0.21116\n",
      "[39]\ttrain-rmse:0.14377\teval-rmse:0.21126\n",
      "[40]\ttrain-rmse:0.14348\teval-rmse:0.21133\n",
      "[41]\ttrain-rmse:0.14077\teval-rmse:0.21106\n",
      "[42]\ttrain-rmse:0.13818\teval-rmse:0.21179\n",
      "[43]\ttrain-rmse:0.13710\teval-rmse:0.21179\n",
      "[44]\ttrain-rmse:0.13551\teval-rmse:0.21232\n",
      "[45]\ttrain-rmse:0.13183\teval-rmse:0.21272\n",
      "[46]\ttrain-rmse:0.12886\teval-rmse:0.21308\n",
      "[47]\ttrain-rmse:0.12808\teval-rmse:0.21310\n",
      "[48]\ttrain-rmse:0.12577\teval-rmse:0.21346\n",
      "[49]\ttrain-rmse:0.12522\teval-rmse:0.21346\n",
      "[50]\ttrain-rmse:0.12338\teval-rmse:0.21470\n",
      "[51]\ttrain-rmse:0.12225\teval-rmse:0.21514\n",
      "[52]\ttrain-rmse:0.12110\teval-rmse:0.21489\n",
      "[53]\ttrain-rmse:0.12036\teval-rmse:0.21475\n",
      "[54]\ttrain-rmse:0.11855\teval-rmse:0.21513\n",
      "[55]\ttrain-rmse:0.11806\teval-rmse:0.21505\n",
      "[56]\ttrain-rmse:0.11692\teval-rmse:0.21504\n",
      "[57]\ttrain-rmse:0.11601\teval-rmse:0.21506\n",
      "[58]\ttrain-rmse:0.11542\teval-rmse:0.21496\n",
      "[59]\ttrain-rmse:0.11535\teval-rmse:0.21496\n",
      "[60]\ttrain-rmse:0.11385\teval-rmse:0.21478\n",
      "[61]\ttrain-rmse:0.11279\teval-rmse:0.21500\n",
      "[62]\ttrain-rmse:0.11039\teval-rmse:0.21531\n",
      "[63]\ttrain-rmse:0.10779\teval-rmse:0.21595\n",
      "[64]\ttrain-rmse:0.10667\teval-rmse:0.21637\n",
      "[65]\ttrain-rmse:0.10660\teval-rmse:0.21634\n",
      "[66]\ttrain-rmse:0.10496\teval-rmse:0.21647\n",
      "[67]\ttrain-rmse:0.10478\teval-rmse:0.21641\n",
      "[68]\ttrain-rmse:0.10304\teval-rmse:0.21659\n",
      "[69]\ttrain-rmse:0.10233\teval-rmse:0.21664\n",
      "[70]\ttrain-rmse:0.10193\teval-rmse:0.21647\n",
      "[71]\ttrain-rmse:0.10015\teval-rmse:0.21659\n",
      "[72]\ttrain-rmse:0.09802\teval-rmse:0.21676\n",
      "[73]\ttrain-rmse:0.09755\teval-rmse:0.21671\n",
      "[74]\ttrain-rmse:0.09714\teval-rmse:0.21675\n",
      "[75]\ttrain-rmse:0.09539\teval-rmse:0.21694\n",
      "[76]\ttrain-rmse:0.09508\teval-rmse:0.21733\n",
      "[77]\ttrain-rmse:0.09452\teval-rmse:0.21725\n",
      "[78]\ttrain-rmse:0.09373\teval-rmse:0.21725\n",
      "[79]\ttrain-rmse:0.09237\teval-rmse:0.21755\n",
      "[80]\ttrain-rmse:0.09091\teval-rmse:0.21762\n",
      "[81]\ttrain-rmse:0.08988\teval-rmse:0.21775\n",
      "[82]\ttrain-rmse:0.08902\teval-rmse:0.21806\n",
      "[83]\ttrain-rmse:0.08791\teval-rmse:0.21809\n",
      "[84]\ttrain-rmse:0.08661\teval-rmse:0.21862\n",
      "[85]\ttrain-rmse:0.08592\teval-rmse:0.21854\n",
      "[86]\ttrain-rmse:0.08577\teval-rmse:0.21871\n",
      "[87]\ttrain-rmse:0.08528\teval-rmse:0.21905\n",
      "[88]\ttrain-rmse:0.08436\teval-rmse:0.21933\n",
      "[89]\ttrain-rmse:0.08381\teval-rmse:0.21921\n",
      "[90]\ttrain-rmse:0.08298\teval-rmse:0.21950\n",
      "[91]\ttrain-rmse:0.08248\teval-rmse:0.21948\n",
      "[92]\ttrain-rmse:0.08188\teval-rmse:0.21940\n",
      "[93]\ttrain-rmse:0.08177\teval-rmse:0.21945\n",
      "[94]\ttrain-rmse:0.08141\teval-rmse:0.21950\n",
      "[95]\ttrain-rmse:0.08116\teval-rmse:0.21971\n",
      "[96]\ttrain-rmse:0.08102\teval-rmse:0.21998\n",
      "[97]\ttrain-rmse:0.08083\teval-rmse:0.22014\n",
      "[98]\ttrain-rmse:0.08024\teval-rmse:0.22011\n",
      "[99]\ttrain-rmse:0.08014\teval-rmse:0.22025\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Clear overfitting occurring as our train rmse goes down but our eval rmse goes up.",
   "id": "1155d4ee7e42860b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:54:59.211462Z",
     "start_time": "2025-12-16T23:54:58.672972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params['eval_metric']='aucpr'\n",
    "#auc is a good metric for classification which is the problem we are trying to solve and it is sensitive to false negatives i.e. missing a stroke which we do not want occuring\n",
    "bst1=xgb.train(params=params, dtrain=dTrain, num_boost_round=100, evals=evalList, verbose_eval=True)"
   ],
   "id": "b102a2ec270322f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-aucpr:0.28578\teval-aucpr:0.20355\n",
      "[1]\ttrain-aucpr:0.40758\teval-aucpr:0.18801\n",
      "[2]\ttrain-aucpr:0.44578\teval-aucpr:0.19553\n",
      "[3]\ttrain-aucpr:0.46944\teval-aucpr:0.19111\n",
      "[4]\ttrain-aucpr:0.49687\teval-aucpr:0.20160\n",
      "[5]\ttrain-aucpr:0.52073\teval-aucpr:0.22373\n",
      "[6]\ttrain-aucpr:0.55634\teval-aucpr:0.21643\n",
      "[7]\ttrain-aucpr:0.57205\teval-aucpr:0.21640\n",
      "[8]\ttrain-aucpr:0.59467\teval-aucpr:0.22401\n",
      "[9]\ttrain-aucpr:0.60335\teval-aucpr:0.21595\n",
      "[10]\ttrain-aucpr:0.61445\teval-aucpr:0.21053\n",
      "[11]\ttrain-aucpr:0.63836\teval-aucpr:0.20160\n",
      "[12]\ttrain-aucpr:0.64307\teval-aucpr:0.20710\n",
      "[13]\ttrain-aucpr:0.64541\teval-aucpr:0.20583\n",
      "[14]\ttrain-aucpr:0.65381\teval-aucpr:0.20348\n",
      "[15]\ttrain-aucpr:0.66102\teval-aucpr:0.20541\n",
      "[16]\ttrain-aucpr:0.66666\teval-aucpr:0.20442\n",
      "[17]\ttrain-aucpr:0.67090\teval-aucpr:0.20488\n",
      "[18]\ttrain-aucpr:0.68740\teval-aucpr:0.20506\n",
      "[19]\ttrain-aucpr:0.69941\teval-aucpr:0.20039\n",
      "[20]\ttrain-aucpr:0.71265\teval-aucpr:0.19849\n",
      "[21]\ttrain-aucpr:0.72171\teval-aucpr:0.19972\n",
      "[22]\ttrain-aucpr:0.73568\teval-aucpr:0.19443\n",
      "[23]\ttrain-aucpr:0.75324\teval-aucpr:0.19478\n",
      "[24]\ttrain-aucpr:0.75857\teval-aucpr:0.19135\n",
      "[25]\ttrain-aucpr:0.78213\teval-aucpr:0.19232\n",
      "[26]\ttrain-aucpr:0.78635\teval-aucpr:0.18993\n",
      "[27]\ttrain-aucpr:0.79701\teval-aucpr:0.18772\n",
      "[28]\ttrain-aucpr:0.79983\teval-aucpr:0.18903\n",
      "[29]\ttrain-aucpr:0.81326\teval-aucpr:0.20376\n",
      "[30]\ttrain-aucpr:0.81643\teval-aucpr:0.20236\n",
      "[31]\ttrain-aucpr:0.81973\teval-aucpr:0.20221\n",
      "[32]\ttrain-aucpr:0.82706\teval-aucpr:0.20196\n",
      "[33]\ttrain-aucpr:0.82987\teval-aucpr:0.20327\n",
      "[34]\ttrain-aucpr:0.83817\teval-aucpr:0.20976\n",
      "[35]\ttrain-aucpr:0.84740\teval-aucpr:0.20739\n",
      "[36]\ttrain-aucpr:0.84895\teval-aucpr:0.20844\n",
      "[37]\ttrain-aucpr:0.85400\teval-aucpr:0.20062\n",
      "[38]\ttrain-aucpr:0.85557\teval-aucpr:0.19960\n",
      "[39]\ttrain-aucpr:0.86831\teval-aucpr:0.19956\n",
      "[40]\ttrain-aucpr:0.87049\teval-aucpr:0.19921\n",
      "[41]\ttrain-aucpr:0.88800\teval-aucpr:0.18023\n",
      "[42]\ttrain-aucpr:0.90013\teval-aucpr:0.17915\n",
      "[43]\ttrain-aucpr:0.90553\teval-aucpr:0.19198\n",
      "[44]\ttrain-aucpr:0.91134\teval-aucpr:0.17756\n",
      "[45]\ttrain-aucpr:0.92637\teval-aucpr:0.19688\n",
      "[46]\ttrain-aucpr:0.93736\teval-aucpr:0.20072\n",
      "[47]\ttrain-aucpr:0.93969\teval-aucpr:0.19481\n",
      "[48]\ttrain-aucpr:0.94840\teval-aucpr:0.18741\n",
      "[49]\ttrain-aucpr:0.94972\teval-aucpr:0.18822\n",
      "[50]\ttrain-aucpr:0.95558\teval-aucpr:0.17833\n",
      "[51]\ttrain-aucpr:0.95766\teval-aucpr:0.17502\n",
      "[52]\ttrain-aucpr:0.96032\teval-aucpr:0.17626\n",
      "[53]\ttrain-aucpr:0.96136\teval-aucpr:0.17824\n",
      "[54]\ttrain-aucpr:0.96614\teval-aucpr:0.16332\n",
      "[55]\ttrain-aucpr:0.96653\teval-aucpr:0.17923\n",
      "[56]\ttrain-aucpr:0.96944\teval-aucpr:0.17888\n",
      "[57]\ttrain-aucpr:0.97164\teval-aucpr:0.17786\n",
      "[58]\ttrain-aucpr:0.97168\teval-aucpr:0.17983\n",
      "[59]\ttrain-aucpr:0.97199\teval-aucpr:0.18033\n",
      "[60]\ttrain-aucpr:0.97458\teval-aucpr:0.18007\n",
      "[61]\ttrain-aucpr:0.97602\teval-aucpr:0.17558\n",
      "[62]\ttrain-aucpr:0.98001\teval-aucpr:0.17531\n",
      "[63]\ttrain-aucpr:0.98345\teval-aucpr:0.17677\n",
      "[64]\ttrain-aucpr:0.98503\teval-aucpr:0.17311\n",
      "[65]\ttrain-aucpr:0.98518\teval-aucpr:0.17313\n",
      "[66]\ttrain-aucpr:0.98683\teval-aucpr:0.17300\n",
      "[67]\ttrain-aucpr:0.98696\teval-aucpr:0.17396\n",
      "[68]\ttrain-aucpr:0.98941\teval-aucpr:0.17143\n",
      "[69]\ttrain-aucpr:0.98977\teval-aucpr:0.17114\n",
      "[70]\ttrain-aucpr:0.98989\teval-aucpr:0.17172\n",
      "[71]\ttrain-aucpr:0.99148\teval-aucpr:0.17230\n",
      "[72]\ttrain-aucpr:0.99247\teval-aucpr:0.17276\n",
      "[73]\ttrain-aucpr:0.99270\teval-aucpr:0.17233\n",
      "[74]\ttrain-aucpr:0.99290\teval-aucpr:0.17003\n",
      "[75]\ttrain-aucpr:0.99387\teval-aucpr:0.17198\n",
      "[76]\ttrain-aucpr:0.99383\teval-aucpr:0.17000\n",
      "[77]\ttrain-aucpr:0.99396\teval-aucpr:0.17010\n",
      "[78]\ttrain-aucpr:0.99495\teval-aucpr:0.16854\n",
      "[79]\ttrain-aucpr:0.99513\teval-aucpr:0.16825\n",
      "[80]\ttrain-aucpr:0.99612\teval-aucpr:0.16797\n",
      "[81]\ttrain-aucpr:0.99693\teval-aucpr:0.16988\n",
      "[82]\ttrain-aucpr:0.99719\teval-aucpr:0.16704\n",
      "[83]\ttrain-aucpr:0.99745\teval-aucpr:0.16582\n",
      "[84]\ttrain-aucpr:0.99799\teval-aucpr:0.16487\n",
      "[85]\ttrain-aucpr:0.99809\teval-aucpr:0.16274\n",
      "[86]\ttrain-aucpr:0.99816\teval-aucpr:0.16262\n",
      "[87]\ttrain-aucpr:0.99821\teval-aucpr:0.16200\n",
      "[88]\ttrain-aucpr:0.99838\teval-aucpr:0.16259\n",
      "[89]\ttrain-aucpr:0.99855\teval-aucpr:0.14775\n",
      "[90]\ttrain-aucpr:0.99891\teval-aucpr:0.14688\n",
      "[91]\ttrain-aucpr:0.99893\teval-aucpr:0.14735\n",
      "[92]\ttrain-aucpr:0.99906\teval-aucpr:0.14693\n",
      "[93]\ttrain-aucpr:0.99908\teval-aucpr:0.14710\n",
      "[94]\ttrain-aucpr:0.99906\teval-aucpr:0.16060\n",
      "[95]\ttrain-aucpr:0.99906\teval-aucpr:0.15884\n",
      "[96]\ttrain-aucpr:0.99906\teval-aucpr:0.15902\n",
      "[97]\ttrain-aucpr:0.99906\teval-aucpr:0.15812\n",
      "[98]\ttrain-aucpr:0.99922\teval-aucpr:0.15900\n",
      "[99]\ttrain-aucpr:0.99922\teval-aucpr:0.15868\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Defining Metrics",
   "id": "5a716bcf8c318c36"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Defining Computational and Environmental Metrics for XGBoost\n",
    "\n",
    "To evaluate the computational efficiency and environmental impact of the boosted tree model (XGBoost), a set of model-appropriate metrics was defined and implemented directly within the training and tuning workflow. While many efficiency and sustainability metrics apply broadly across machine learning models, neural-network-specific measures require adaptation when applied to tree-based algorithms. To enable meaningful comparison with neural network approaches, floating point operations (FLOPs) were explicitly estimated for boosted decision trees using a traversal-based approximation that reflects the structure of tree ensembles.\n",
    "\n",
    "All metrics were collected programmatically during execution using a custom monitoring utility. As a result, the reported values reflect the full cost of model development, including cross-validated grid search, learning rate tuning with early stopping, and final model training, rather than only the cost of the final fitted model.\n",
    "\n",
    "### Computational Metrics\n",
    "\n",
    "The following metrics were used to quantify computational efficiency for the boosted tree model:\n",
    "\n",
    "- Training time\n",
    "- Average CPU utilization\n",
    "- Peak CPU utilization\n",
    "- RAM usage\n",
    "- Floating point operations (FLOPs)\n",
    "\n",
    "### Computational Metric Definitions\n",
    "\n",
    "Training time (s)\n",
    "= End time − Start time\n",
    "\n",
    "Training time was measured as wall-clock time spanning the entire model development process, including hyperparameter search and final training.\n",
    "\n",
    "Average CPU utilization (%)\n",
    "= (Sum of CPU utilization samples) / (Number of samples)\n",
    "\n",
    "CPU usage was sampled periodically during execution using system-level monitoring, capturing average utilization across the full training and tuning period.\n",
    "\n",
    "Peak CPU utilization (%)\n",
    "= Maximum value observed across all CPU samples\n",
    "\n",
    "Peak CPU utilization represents the highest instantaneous CPU usage observed during model training or evaluation.\n",
    "\n",
    "RAM usage (GB)\n",
    "= Resident Set Size (bytes) / 1,000,000,000\n",
    "\n",
    "Memory usage was measured using the process resident set size, capturing the total memory footprint of the Python process, including the dataset stored in XGBoost’s DMatrix format and the learned tree structures.\n",
    "\n",
    "### FLOPs Estimation for Boosted Trees\n",
    "\n",
    "Boosted trees do not rely on dense matrix multiplications as neural networks do. Instead, their computational cost is dominated by decision node evaluations during tree traversal and gradient-based updates during training. FLOPs were therefore estimated using a traversal-based approximation aligned with the structure of boosted decision trees.\n",
    "\n",
    "FLOPs per training run\n",
    "= Number of samples × Number of boosting rounds × Average tree depth × k × C\n",
    "\n",
    "where:\n",
    "\n",
    "- Average tree depth serves as a proxy for the number of node evaluations per tree\n",
    "- k represents the number of floating point operations per node evaluation, accounting for comparison and branching\n",
    "- C is a training multiplier accounting for forward traversal, gradient computation, and tree update operations\n",
    "\n",
    "### Cumulative FLOPs Accounting\n",
    "\n",
    "To accurately capture total computational cost, FLOPs were accumulated across all stages of model development.\n",
    "\n",
    "Grid search FLOPs were estimated by multiplying the FLOPs of a single model by the number of hyperparameter combinations and the number of cross-validation folds.\n",
    "\n",
    "Learning rate and early stopping FLOPs were added based on the actual number of boosting rounds executed before early stopping for each configuration.\n",
    "\n",
    "Final model FLOPs were computed separately using the same formulation, allowing the cost of the selected model to be reported independently while still retaining the cumulative FLOPs from tuning.\n",
    "\n",
    "This approach ensures that the reported FLOPs represent the full computational expense of model selection rather than underestimating cost by considering only the final model.\n",
    "\n",
    "### Environmental Metrics\n",
    "\n",
    "The following metrics were used to estimate the environmental impact of training the boosted tree model:\n",
    "\n",
    "- Energy consumption\n",
    "- CO2 emissions\n",
    "- Water consumption\n",
    "\n",
    "### Environmental Metric Formulas\n",
    "\n",
    "Energy consumption (Wh)\n",
    "= (Average power usage × Training time) / 3600\n",
    "\n",
    "Training was performed on a personal computing device using CPU-based execution. A constant average power draw of 3.6 W was assumed based on system-level measurements.\n",
    "\n",
    "CO2 emissions (g)\n",
    "= Energy consumption × Carbon intensity\n",
    "\n",
    "Carbon intensity was assumed to be 0.408 gCO2/Wh, corresponding to the regional electricity grid average.\n",
    "\n",
    "Water consumption (mL)\n",
    "= Energy consumption × Water intensity\n",
    "\n",
    "A water intensity of 2.18 L/kWh (2.18 mL/Wh) was used based on U.S. national averages.\n",
    "\n",
    "### Notes on Environmental Assumptions\n",
    "\n",
    "Training was performed on a personal laptop rather than in a data center environment, so power usage effectiveness was not included. Region-specific carbon and water intensity data were unavailable, and national averages were applied. Cooling and infrastructure overhead factors were omitted for simplicity.\n",
    "\n"
   ],
   "id": "271beabe79ee18c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T23:54:59.232115Z",
     "start_time": "2025-12-16T23:54:59.220713Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import time, psutil, os, threading\n",
    "import numpy as np\n",
    "\n",
    "class TreeMetrics:\n",
    "    def __init__(self, name=\"XGBoost\", carbonIntensity=0.408, waterIntensity=2.18):\n",
    "        self.name = name\n",
    "        self.carbonIntensity = carbonIntensity\n",
    "        self.waterIntensity = waterIntensity\n",
    "        self.process = psutil.Process(os.getpid())\n",
    "        self.cpuSamples = []\n",
    "        self.totalFlops = 0\n",
    "\n",
    "    # Runtime monitoring\n",
    "    def start(self):\n",
    "        self.startTime = time.time()\n",
    "        self.cpuSamples = []\n",
    "        self.monitoring = True\n",
    "        self.monitor_thread = threading.Thread(\n",
    "            target=self._monitor_cpu, daemon=True\n",
    "        )\n",
    "        self.monitor_thread.start()\n",
    "        print(f\"Tracking: {self.name}\")\n",
    "\n",
    "    def _monitor_cpu(self):\n",
    "        while self.monitoring:\n",
    "            self.cpuSamples.append(psutil.cpu_percent(interval=0.1))\n",
    "            time.sleep(0.5)\n",
    "\n",
    "    def stop(self):\n",
    "        self.endTime = time.time()\n",
    "        self.monitoring = False\n",
    "        if hasattr(self, \"monitor_thread\"):\n",
    "            self.monitor_thread.join(timeout=2.0)\n",
    "\n",
    "    # FLOPs accounting\n",
    "    \"\"\"FLOPs for XGBoost training:num_samples × num_trees × depth × node_ops × train_ops\"\"\"\n",
    "    def addFlopsTraining(\n",
    "        self,numSamples,numBoostRounds,avgTreeDepth,kNode=4,cTrain=3):\n",
    "        flops = (\n",
    "            numSamples *\n",
    "            numBoostRounds *\n",
    "            avgTreeDepth *\n",
    "            kNode *\n",
    "            cTrain\n",
    "        )\n",
    "        self.totalFlops += flops\n",
    "\n",
    "    def addFlopsGridsearch(\n",
    "        self,paramGrid,numEstimators,cv,numSamples,kNode=4,cTrain=3):\n",
    "        numParamCombos = 1\n",
    "        for v in paramGrid.values():\n",
    "            numParamCombos *= len(v)\n",
    "\n",
    "        avgDepth = np.mean(paramGrid[\"max_depth\"])\n",
    "\n",
    "        flopsPerModel = (\n",
    "            numSamples *\n",
    "            numEstimators *\n",
    "            avgDepth *\n",
    "            kNode *\n",
    "            cTrain\n",
    "        )\n",
    "\n",
    "        total = flopsPerModel * numParamCombos * cv\n",
    "        self.totalFlops += total\n",
    "        \"\"\"Computes FLOPs for training the final selected XGBoost model only.\n",
    "        kNode : intFLOPs per node evaluation (comparison + branching)\n",
    "        CTrain :Training multiplier (forward + gradient + update)\"\"\"\n",
    "    def computeFinalModelFlops(\n",
    "        self,numSamples,numBoostRounds,avgTreeDepth,kNode=4,cTrain=3):\n",
    "\n",
    "        flops = (\n",
    "            numSamples *\n",
    "            numBoostRounds *\n",
    "            avgTreeDepth *\n",
    "            kNode *\n",
    "            cTrain\n",
    "        )\n",
    "\n",
    "        return flops\n",
    "    # Final report\n",
    "    def report(self, powerUsage=3.6):\n",
    "        trainingTime = self.endTime - self.startTime\n",
    "\n",
    "        avgCpu = (\n",
    "            sum(self.cpuSamples) / len(self.cpuSamples)\n",
    "            if self.cpuSamples else 0\n",
    "        )\n",
    "        peak_cpu = max(self.cpuSamples) if self.cpuSamples else 0\n",
    "\n",
    "        energy = powerUsage * trainingTime / 3600\n",
    "        co2 = energy * self.carbonIntensity\n",
    "        water = energy * self.waterIntensity\n",
    "\n",
    "        print(\"\\n===== FINAL COMPUTE REPORT =====\")\n",
    "        print(f\"Training time: {trainingTime:.2f}s ({trainingTime/60:.2f} min)\")\n",
    "        print(f\"Average CPU Utilization: {avgCpu:.2f}%\")\n",
    "        print(f\"Peak CPU Utilization: {peak_cpu:.2f}%\")\n",
    "        print(f\"RAM Usage: {self.process.memory_info().rss/1e9:.2f} GB\")\n",
    "        print(f\"Total Estimated FLOPs: {self.totalFlops/1e9:.3f} GFLOPs\")\n",
    "        print(f\"Energy Consumption: {energy:.6f} Wh\")\n",
    "        print(f\"CO₂ Emissions: {co2:.6f} g\")\n",
    "        print(f\"Water Consumption: {water:.4f} mL\")\n"
   ],
   "id": "b816eefbe6d6bf2c",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model Creation and Tuning",
   "id": "a6d0801b9d39c528"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "### Cross-Validated Hyperparameter Optimization\n",
    "\n",
    "This section performs structured hyperparameter tuning using cross-validated grid search to identify robust tree-level and regularization settings for the XGBoost model.\n",
    "\n",
    "Before model tuning begins, computational tracking is initialized using the TreeMetrics utility. This enables measurement of training time, CPU utilization, and cumulative FLOPs across all tuning stages, ensuring that the full cost of model selection is accounted for rather than only the final model fit.\n",
    "\n",
    "Class imbalance is explicitly handled prior to training by computing the ratio of negative to positive samples in the training set. This ratio is used to set the scale_pos_weight parameter, which increases the penalty for misclassifying stroke cases and encourages the model to allocate more capacity to the minority class without altering the underlying data distribution.\n",
    "\n",
    "An XGBoost classifier is then instantiated using conservative baseline settings. The exact tree construction method is selected due to the moderate dataset size, allowing precise split evaluation. Subsampling and column sampling are fixed below one to reduce variance and mitigate overfitting, while the evaluation metric is set to PR-AUC to align with the imbalanced classification objective.\n",
    "\n",
    "Hyperparameter tuning is conducted using GridSearchCV with five-fold cross-validation. The parameter grid focuses on tree depth, minimum child weight, loss reduction threshold, and L1 and L2 regularization strengths. These parameters directly control model complexity and generalization behavior, making them appropriate candidates for structured search. Average precision is used as the scoring metric to remain consistent with the PR-AUC optimization goal.\n",
    "\n",
    "After fitting the grid search, the total computational cost of this stage is estimated by explicitly adding FLOPs associated with all cross-validated model fits. This calculation accounts for the number of parameter combinations, the number of cross-validation folds, the number of estimators per model, and the training set size. By incorporating grid search FLOPs into the overall accounting, the reported compute metrics accurately reflect the full cost of model selection rather than only the final chosen configuration.\n"
   ],
   "id": "ba3b500035afe04e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T00:02:31.437681Z",
     "start_time": "2025-12-16T23:54:59.243673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "metrics = TreeMetrics(name=\"XGBoost Final Model\")\n",
    "metrics.start()\n",
    "\"\"\"scale_pos_weights Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances)\"\"\"\n",
    "neg = (trainLabels == 0).sum()\n",
    "pos = (trainLabels == 1).sum()\n",
    "\n",
    "scale_pos_weight = neg / pos\n",
    "print(scale_pos_weight)\n",
    "\n",
    "bstSklearnWrapper=xgb.XGBClassifier(n_estimators=100, max_depth=6,tree_method='exact',learning_rate=0.1,objective=\"binary:logistic\", booster='gbtree',gamma=0,min_child_weight=3,max_delta_step=1,subsample=0.7,colsample_bytree=0.7,colsample_bylevel=1,colsample_bynode=1,reg_alpha=0,reg_lambda=1,scale_pos_weight=scale_pos_weight,random_state=42,eval_metric='aucpr')\n",
    "paramGrid={'max_depth':[3,4,5,6],'max_delta_step':[1,2,3,4],'min_child_weight':[2,3,4],'gamma':[0,1],'reg_alpha':[0,1],\"reg_lambda\":[0,1]}\n",
    "grid=GridSearchCV(estimator=bstSklearnWrapper,param_grid=paramGrid,cv=5,scoring='average_precision',verbose=1)\n",
    "grid.fit(trainFeatures, trainLabels)\n",
    "metrics.addFlopsGridsearch(\n",
    "    paramGrid=paramGrid,\n",
    "    numEstimators=bstSklearnWrapper.n_estimators,\n",
    "    cv=grid.cv,\n",
    "    numSamples=trainFeatures.shape[0]\n",
    ")\n",
    "\n"
   ],
   "id": "4a74b8ff56414148",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracking: XGBoost Final Model\n",
      "19.542713567839197\n",
      "Fitting 5 folds for each of 384 candidates, totalling 1920 fits\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Extracting the Best Grid Search Configuration\n",
    "\n",
    "After completing cross-validated hyperparameter tuning, the best-performing model configuration is extracted from the grid search. The best estimator represents the combination of tree structure and regularization parameters that achieved the highest average precision across validation folds. The corresponding parameter values and cross-validation score are printed to document the selected configuration and its performance.\n",
    "\n",
    "The full parameter dictionary of the selected estimator is then retrieved. From this dictionary, a streamlined set of core training parameters is constructed. This base parameter set includes the objective function, evaluation metric, tree construction method, depth and regularization settings, sampling behavior, class imbalance weighting, and random seed. Learning rate and number of boosting rounds are intentionally excluded at this stage, as these parameters are tuned separately due to their strong interaction.\n",
    "\n",
    "This separation allows the structural complexity of the model to be fixed before optimizing training dynamics."
   ],
   "id": "d4eef98465a622c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T00:02:31.475493Z",
     "start_time": "2025-12-17T00:02:31.469304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bestModel=grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n",
    "skParams = grid.best_estimator_.get_params()\n",
    "baseParams = {\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": \"aucpr\",\n",
    "    \"tree_method\": skParams[\"tree_method\"],\n",
    "    \"booster\": skParams[\"booster\"],\n",
    "    \"max_depth\": skParams[\"max_depth\"],\n",
    "    \"min_child_weight\": skParams[\"min_child_weight\"],\n",
    "    \"max_delta_step\": skParams[\"max_delta_step\"],\n",
    "    \"subsample\": skParams[\"subsample\"],\n",
    "    \"colsample_bytree\": skParams[\"colsample_bytree\"],\n",
    "    \"gamma\": skParams[\"gamma\"],\n",
    "    \"alpha\": skParams[\"reg_alpha\"],\n",
    "    \"lambda\": skParams[\"reg_lambda\"],\n",
    "    \"scale_pos_weight\": skParams[\"scale_pos_weight\"],\n",
    "    \"seed\": skParams[\"random_state\"],\n",
    "}\n",
    "\n"
   ],
   "id": "54dc4d29874d421e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0, 'max_delta_step': 3, 'max_depth': 5, 'min_child_weight': 3, 'reg_alpha': 1, 'reg_lambda': 0}\n",
      "0.23254648276381293\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Validation Split for Learning Rate and Boosting Round Search\n",
    "\n",
    "To tune the learning rate and number of estimators, the training data is further split into a reduced training subset and a held-out validation subset. Stratification is applied to preserve the original class imbalance, ensuring that stroke cases remain adequately represented during evaluation.\n",
    "\n",
    "Both subsets are converted into XGBoost DMatrix objects, which provide optimized memory handling and efficient computation during training."
   ],
   "id": "82c393db86c95830"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T00:02:31.528644Z",
     "start_time": "2025-12-17T00:02:31.518796Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#splitting the train data into train data and validation data\n",
    "trainFeatures, valFeatures, trainLabels, valLabels = train_test_split(\n",
    "    trainFeatures, trainLabels, test_size=0.2, random_state=42,stratify=trainLabels)\n",
    "dVal = xgb.DMatrix(valFeatures, valLabels)\n",
    "dTrain = xgb.DMatrix(trainFeatures, trainLabels)"
   ],
   "id": "17f3a3a98e26b9a7",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Joint Optimization of Learning Rate and Early Stopping\n",
    "\n",
    "Learning rate and number of estimators are optimized jointly using a nested loop over candidate learning rates and early stopping patience values. For each combination, the model is trained using a large upper bound on the number of boosting rounds, while early stopping halts training once validation performance stops improving.\n",
    "\n",
    "This approach allows the effective number of trees to be determined dynamically rather than fixed in advance, preventing unnecessary training and reducing overfitting risk.\n",
    "\n",
    "For each trained model, computational cost is explicitly tracked by adding estimated FLOPs based on the number of samples, the number of trees actually trained before early stopping, and the fixed tree depth. This ensures that the total computational footprint of the learning rate search is included in the final metrics.\n",
    "\n",
    "Validation predictions are generated for each model, and PR-AUC is computed on the validation set. Each result, including the learning rate, early stopping patience, effective number of estimators, validation PR-AUC, and trained model, is stored for comparison.\n",
    "\n",
    "Once all learning rate and early stopping combinations have been evaluated, the model achieving the highest validation PR-AUC is selected as the final model. This selection criterion prioritizes performance on the minority stroke class and aligns with the overall objective of minimizing false negatives.\n",
    "\n",
    "The final parameter set is assembled by combining the previously fixed base parameters with the selected learning rate. This configuration is used for final evaluation on the test set and for reporting computational and environmental metrics."
   ],
   "id": "100abf1a885ccbf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T00:02:50.027363Z",
     "start_time": "2025-12-17T00:02:31.537842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.metrics import average_precision_score\n",
    "learningRates = [0.001,0.003,0.005,0.008,0.01, 0.03, 0.05, 0.08, 0.1,0.3]\n",
    "earlyStoppingRounds = [20,30,40,50,60,70,80,100]\n",
    "\n",
    "results = []\n",
    "for learningRate in learningRates:\n",
    "    for patience in earlyStoppingRounds:\n",
    "        params = baseParams.copy()\n",
    "        params['learning_rate'] = learningRate\n",
    "\n",
    "        booster = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=dTrain,\n",
    "            num_boost_round=2000,\n",
    "            evals=[(dVal, \"val\")],\n",
    "            early_stopping_rounds=patience,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        metrics.addFlopsTraining(\n",
    "            numSamples=dTrain.num_row(),\n",
    "            numBoostRounds=booster.best_iteration + 1,\n",
    "            avgTreeDepth=baseParams[\"max_depth\"]\n",
    "        )\n",
    "\n",
    "        yValProba = booster.predict(dVal)\n",
    "        pr_auc = average_precision_score(valLabels, yValProba)\n",
    "\n",
    "        results.append({\n",
    "            \"learningRate\": learningRate,\n",
    "            \"earlyStoppingRounds\": patience,\n",
    "            \"bestNumEstimators\": booster.best_iteration + 1,\n",
    "            \"valPrAuc\": pr_auc,\n",
    "            \"model\": booster\n",
    "        })\n",
    "bestResult = max(results, key=lambda x: x[\"valPrAuc\"])\n",
    "print(bestResult)\n",
    "bestModel=bestResult[\"model\"]\n",
    "bestParamsFinal = baseParams.copy()\n",
    "bestParamsFinal[\"learning_rate\"] = bestResult[\"learningRate\"]"
   ],
   "id": "70ade23e8abaaea3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learningRate': 0.008, 'earlyStoppingRounds': 50, 'bestNumEstimators': 26, 'valPrAuc': 0.18387008596980342, 'model': <xgboost.core.Booster object at 0x125df2c60>}\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Decision Threshold Optimization\n",
    "\n",
    "After selecting the best-performing boosted tree model, the final tuning step focuses on optimizing the decision threshold used to convert predicted probabilities into binary class labels. By default, probabilistic classifiers use a threshold of 0.5; however, in highly imbalanced medical datasets, this default choice often leads to an unacceptable number of false negatives.\n",
    "\n",
    "The trained model is first used to generate predicted probabilities for the validation set. Rather than committing to a single threshold, a range of candidate thresholds between 0.01 and 0.99 is evaluated. For each threshold, predicted probabilities are converted into class labels, and precision, recall, and F1-score are computed.\n",
    "\n",
    "These metrics are recorded for each threshold to characterize the trade-off between correctly identifying stroke cases and minimizing false alarms. Because missed stroke predictions are particularly costly, recall is treated as a primary constraint. Only thresholds that achieve a recall of at least 0.85 on the validation set are considered viable.\n",
    "\n",
    "Among this filtered set of thresholds, the threshold that maximizes precision is selected. This approach ensures that the final classifier maintains high sensitivity to stroke cases while improving precision as much as possible under the recall constraint.\n",
    "\n",
    "Once the optimal threshold is identified, model training and tuning are complete, and computational monitoring is stopped. The selected threshold is then used for final evaluation on the held-out test set."
   ],
   "id": "ecaa40f4726c6073"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T00:02:50.770720Z",
     "start_time": "2025-12-17T00:02:50.061508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "yValProba = bestModel.predict(dVal)\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "thresholds = np.linspace(0.01, 0.99, 99)\n",
    "thresholdResults = []\n",
    "\n",
    "for t in thresholds:\n",
    "    yValPred = (yValProba >= t).astype(int)\n",
    "\n",
    "    precision = precision_score(valLabels, yValPred, zero_division=0)\n",
    "    recall    = recall_score(valLabels, yValPred, zero_division=0)\n",
    "    f1        = f1_score(valLabels, yValPred, zero_division=0)\n",
    "\n",
    "    thresholdResults.append({\n",
    "        \"threshold\": t,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    })\n",
    "recallTarget=0.85\n",
    "\n",
    "thresholdDf = pd.DataFrame(thresholdResults)\n",
    "filtered = thresholdDf[thresholdDf[\"recall\"] >= recallTarget]\n",
    "bestRow = filtered.loc[filtered[\"precision\"].idxmax()]\n",
    "bestThreshold = bestRow[\"threshold\"]\n",
    "\n",
    "print(\"Best threshold (max precision):\")\n",
    "print(bestRow)\n",
    "metrics.stop()\n"
   ],
   "id": "5c74be7113bc8bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best threshold (max precision):\n",
      "threshold    0.470000\n",
      "precision    0.143460\n",
      "recall       0.850000\n",
      "f1           0.245487\n",
      "Name: 46, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Final Model Evaluation and Computational Summary\n",
    "\n",
    "After completing model tuning and threshold selection, the final boosted tree model is evaluated on the held-out test set to assess its generalization performance. Predicted probabilities are first generated using the optimized model, and the previously selected decision threshold is applied to convert these probabilities into binary class predictions.\n",
    "\n",
    "Model performance is evaluated using metrics appropriate for imbalanced classification tasks. The area under the precision–recall curve (PR-AUC) is reported as the primary metric, as it directly reflects performance on the minority stroke class. For completeness, the ROC-AUC score is also reported to provide a complementary view of ranking performance across all thresholds. A full classification report is printed, summarizing precision, recall, and F1-score for each class.\n",
    "\n",
    "Once predictive performance is evaluated, a final computational and environmental report is generated using the TreeMetrics utility. This report summarizes total runtime, CPU utilization, memory usage, cumulative FLOPs across all training and tuning stages, and estimated energy, carbon, and water consumption.\n",
    "\n",
    "In addition to the cumulative metrics, FLOPs are computed separately for the final selected model only. This calculation isolates the computational cost associated with training the final model configuration, excluding the overhead introduced by grid search, learning-rate exploration, and early stopping experiments. The final model’s FLOPs are reported both in absolute terms and as a percentage of the total FLOPs incurred throughout the entire training and tuning process.\n",
    "\n",
    "To provide an interpretable view of classification outcomes, a confusion matrix is constructed using test set predictions. The matrix displays counts of true positives, true negatives, false positives, and false negatives using human-readable class labels. This visualization highlights the model’s ability to identify stroke cases while contextualizing the trade-off between sensitivity and specificity.\n",
    "\n",
    "Together, these results provide a comprehensive view of both predictive performance and computational cost for the final XGBoost model, enabling transparen\n"
   ],
   "id": "ae1be6ad65a25c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-17T00:02:50.888071Z",
     "start_time": "2025-12-17T00:02:50.780822Z"
    }
   },
   "cell_type": "code",
   "source": [
    "yTestProba = bestModel.predict(dTest)\n",
    "yTestPred  = (yTestProba >= bestThreshold).astype(int)\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "print(\"Test PR-AUC:\", average_precision_score(testLabels, yTestProba))\n",
    "print(\"Test ROC-AUC:\", roc_auc_score(testLabels, yTestProba))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(testLabels, yTestPred, digits=4))\n",
    "\n",
    "metrics.report()\n",
    "finalModelFlops = metrics.computeFinalModelFlops(\n",
    "    numSamples=dTrain.num_row(),\n",
    "    numBoostRounds=bestModel.best_iteration + 1,\n",
    "    avgTreeDepth=bestParamsFinal[\"max_depth\"]\n",
    ")\n",
    "\n",
    "print(\"\\n===== FINAL MODEL ONLY =====\")\n",
    "print(f\"Final Model FLOPs: {finalModelFlops/1e9:.3f} GFLOPs\")\n",
    "print(f\"Final Model Share of Total GFLOPs: {(finalModelFlops / metrics.totalFlops) * 100:.2f}%\")\n",
    "\n",
    "\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=confusion_matrix(testLabels, yTestPred),\n",
    "    display_labels=[\"No Stroke\", \"Stroke\"]\n",
    ")\n",
    "\n",
    "disp.plot(\n",
    "    cmap=\"Blues\",\n",
    "    values_format=\"d\"\n",
    ")\n",
    "\n",
    "plt.title(\"Confusion Matrix – Final XGBoost Model\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "2fa595ca823fb201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PR-AUC: 0.2513923525978292\n",
      "Test ROC-AUC: 0.8470781893004115\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9864    0.7469    0.8501       972\n",
      "           1     0.1399    0.8000    0.2381        50\n",
      "\n",
      "    accuracy                         0.7495      1022\n",
      "   macro avg     0.5631    0.7735    0.5441      1022\n",
      "weighted avg     0.9450    0.7495    0.8202      1022\n",
      "\n",
      "\n",
      "===== FINAL COMPUTE REPORT =====\n",
      "Training time: 471.21s (7.85 min)\n",
      "Average CPU Utilization: 34.32%\n",
      "Peak CPU Utilization: 100.00%\n",
      "RAM Usage: 0.13 GB\n",
      "Total Estimated FLOPs: 42.739 GFLOPs\n",
      "Energy Consumption: 0.471207 Wh\n",
      "CO₂ Emissions: 0.192252 g\n",
      "Water Consumption: 1.0272 mL\n",
      "\n",
      "===== FINAL MODEL ONLY =====\n",
      "Final Model FLOPs: 0.005 GFLOPs\n",
      "Final Model Share of Total GFLOPs: 0.01%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 50
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
