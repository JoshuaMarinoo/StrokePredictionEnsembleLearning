{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Ensemble Learning",
   "id": "575c4cb5c554f2f7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Imports and Setting up the Kaggle API\n",
    "### Create .env File and Set KAGGLE_KEY and KAGGLE_USERNAME as Kaggle Username and Key in .env File\n",
    "### Example:\n",
    "KAGGLE_KEY=API_KEY\n",
    "KAGGLE_USERNAME=USERNAME\n",
    "\n",
    "load_dotenv will take .env and set key pairs as environmental variables in Python"
   ],
   "id": "297190960c102f2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:24:48.852131Z",
     "start_time": "2025-11-28T18:24:48.846835Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import kaggle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "\n"
   ],
   "id": "856954414c93e72",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Setting the API Instance and downloading dataset",
   "id": "6b090646430f7745"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:24:49.321175Z",
     "start_time": "2025-11-28T18:24:48.865034Z"
    }
   },
   "cell_type": "code",
   "source": [
    "apiInstance=kaggle.KaggleApi()\n",
    "apiInstance.dataset_download_files('fedesoriano/stroke-prediction-dataset', unzip=True)"
   ],
   "id": "471110ed8d675f2b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocessing\n",
   "id": "11c341ab6eddc815"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:24:49.349764Z",
     "start_time": "2025-11-28T18:24:49.326637Z"
    }
   },
   "cell_type": "code",
   "source": [
    "strokeData=pd.read_csv('healthcare-dataset-stroke-data.csv')\n",
    "#strokeData.info()\n",
    "strokeDataFeatures=strokeData.iloc[:,1:-1]\n",
    "#iloc[rows,columns] we used : on rows as :specifies a range so a range with no upper or lower bound means taking everyting\n",
    "#1:-1 means a range from 1(dropping our first column) to -1(which really means our last column)\n",
    "#dropping the first column our ID column since it has no predictive power and can potentially cause any learners we use to develop patterns on it\n",
    "#dropping the last column since we only want our features and not the labels\n",
    "strokeDataLabels=strokeData.iloc[:,-1]\n",
    "#getting only the last column as we only want the labels"
   ],
   "id": "5b0679b68ba4edbb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:24:49.536941Z",
     "start_time": "2025-11-28T18:24:49.355006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(strokeData.isnull().any())\n",
    "#BMI is the only column with NaNs\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer=SimpleImputer(strategy='mean')\n",
    "# our BMI column is our 8th column so we want to put that column in the imputer\n",
    "strokeDataFeatures[['bmi']]=pd.DataFrame(imputer.fit_transform(strokeDataFeatures[['bmi']]))\n",
    "print(strokeDataFeatures.isnull().any())"
   ],
   "id": "c0b9f15663beb258",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id                   False\n",
      "gender               False\n",
      "age                  False\n",
      "hypertension         False\n",
      "heart_disease        False\n",
      "ever_married         False\n",
      "work_type            False\n",
      "Residence_type       False\n",
      "avg_glucose_level    False\n",
      "bmi                   True\n",
      "smoking_status       False\n",
      "stroke               False\n",
      "dtype: bool\n",
      "gender               False\n",
      "age                  False\n",
      "hypertension         False\n",
      "heart_disease        False\n",
      "ever_married         False\n",
      "work_type            False\n",
      "Residence_type       False\n",
      "avg_glucose_level    False\n",
      "bmi                  False\n",
      "smoking_status       False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:24:49.577012Z",
     "start_time": "2025-11-28T18:24:49.543190Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#we want to use standard scaler to scale the inputs for the numberical inputs to avoid problems with weights for different model types\n",
    "#standard scaler or minmax scaler are good but we will use StandardScaler\n",
    "#we want to use onehotencoder for categorical columns\n",
    "# that have more than 2 possible awnsers, ordinal or label encoder for categorical columsn that have only 2 possible awnsers and then we want to use the columnTransformer to apply these encoders to the columsn we want to apply them to\n",
    "\n",
    "#these lines help us figure out which columsn need to be onehotencoded and which need to be converted to binary\n",
    "print('Columns to be OneHotEncoded')\n",
    "for column in strokeDataFeatures:\n",
    "    if (strokeDataFeatures[column].nunique()>2) & (strokeDataFeatures[column].dtype == 'object'):\n",
    "        print(f'{column} has unique categories of {strokeDataFeatures[column].unique()}')\n",
    "#gender work_type and smoking_status should be OneHotEncoded to avoid the learners accidentally ranking\n",
    "print('Columns to be converted to Binary')\n",
    "for column in strokeDataFeatures:\n",
    "    if (strokeDataFeatures[column].nunique()==2) & (strokeDataFeatures[column].dtype == 'object'):\n",
    "        print(f'{column} has unique categories of {strokeDataFeatures[column].unique()}')\n",
    "#ever_married and residence_type can be converted to binary 0 and 1 since there are only 2 values\n",
    "\n",
    "#column transformer takes in an array of tuples(each tuples has three values) each tuple is represents an encoder you will use on some columns in the tuples you have three values the first value is some arbitrary name like 'ordinalEncoder' and the second vlaue is the function for the encoder itself like OrdinalEncoder() the third value is a list of the column indices or column names if the data is a dataframe which you want that specific encoder to be used on so in this case we only want our OrdinalEncoder to\n",
    "ct=ColumnTransformer(transformers=[('ordinalEncoder', OrdinalEncoder(), ['ever_married','Residence_type']),('oneHotEncoder', OneHotEncoder(), ['gender','work_type','smoking_status']),('scaler', StandardScaler(),['bmi','avg_glucose_level','age'])],remainder='passthrough')\n",
    "strokeDataFeatures=ct.fit_transform(strokeDataFeatures)\n",
    "print(ct.get_feature_names_out())\n",
    "strokeDataFeatures[0]\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "2120871b07da2b7b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns to be OneHotEncoded\n",
      "gender has unique categories of ['Male' 'Female' 'Other']\n",
      "work_type has unique categories of ['Private' 'Self-employed' 'Govt_job' 'children' 'Never_worked']\n",
      "smoking_status has unique categories of ['formerly smoked' 'never smoked' 'smokes' 'Unknown']\n",
      "Columns to be converted to Binary\n",
      "ever_married has unique categories of ['Yes' 'No']\n",
      "Residence_type has unique categories of ['Urban' 'Rural']\n",
      "['ordinalEncoder__ever_married' 'ordinalEncoder__Residence_type'\n",
      " 'oneHotEncoder__gender_Female' 'oneHotEncoder__gender_Male'\n",
      " 'oneHotEncoder__gender_Other' 'oneHotEncoder__work_type_Govt_job'\n",
      " 'oneHotEncoder__work_type_Never_worked'\n",
      " 'oneHotEncoder__work_type_Private'\n",
      " 'oneHotEncoder__work_type_Self-employed'\n",
      " 'oneHotEncoder__work_type_children'\n",
      " 'oneHotEncoder__smoking_status_Unknown'\n",
      " 'oneHotEncoder__smoking_status_formerly smoked'\n",
      " 'oneHotEncoder__smoking_status_never smoked'\n",
      " 'oneHotEncoder__smoking_status_smokes' 'scaler__bmi'\n",
      " 'scaler__avg_glucose_level' 'scaler__age' 'remainder__hypertension'\n",
      " 'remainder__heart_disease']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 0.        , 1.        , 0.        ,\n",
       "       0.        , 0.        , 1.        , 0.        , 0.        ,\n",
       "       0.        , 1.        , 0.        , 0.        , 1.00123401,\n",
       "       2.70637544, 1.05143428, 0.        , 1.        ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Test-Train Split",
   "id": "48957f813fbc7df3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:24:49.601560Z",
     "start_time": "2025-11-28T18:24:49.594501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainFeatures, testFeatures, trainLabels, testLabels = train_test_split(\n",
    "    strokeDataFeatures, strokeDataLabels, test_size=0.2, random_state=42,stratify=strokeDataLabels)"
   ],
   "id": "6e8c7c74f7fdb67e",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:24:49.610414Z",
     "start_time": "2025-11-28T18:24:49.606956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(trainFeatures[0])\n",
    "print(trainLabels[0])"
   ],
   "id": "6974528e3f2575d7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.          1.          1.          0.          0.          0.\n",
      "  0.          1.          0.          0.          0.          0.\n",
      "  1.          0.          0.54652702 -0.81577711  0.21111428  0.\n",
      "  0.        ]\n",
      "1\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Creation and Setup\n",
    "We want to use XGBoost so we need to convert our dataset from numpy arrays to Dmatrix"
   ],
   "id": "d88e7d519c7caf4a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:24:49.998200Z",
     "start_time": "2025-11-28T18:24:49.627876Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dTrain=xgb.DMatrix(trainFeatures, trainLabels)\n",
    "dTest = xgb.DMatrix(testFeatures, testLabels)\n",
    "#xgboost parameters we will need to optimize to make ours ensemble training better\n",
    "\n",
    "#Tree Booster Parameters\n",
    "\n",
    "#eta is our learning rate\n",
    "#gamma is the minimum loss reduction to make a further leaf partition increasing makes the model less complex\n",
    "#max_depth is max depth of the tree increasing max depth can make the model more complex\n",
    "#min_child_weight relates to the minimum amount of samples can be in a leaf node increasing decrease the model complexity\n",
    "#max_delta_step acts to reduce how fast leaf values can change important for us as the skewed dataset can mean that the tree can change drastically to fit our very low amount of strokes in dataset so by increasing max_delta_step above 0 we can reduce the rate of change of leaf values meaning there is a less change of our skewed dataset from overfitting quickly\n",
    "#subsample tells how much of the dataset to randomly sample before growin tree decreasing number will help reduce overfitting\n",
    "#sampling_method how the dataset samples are chosen uniform means each instance has an equal change of being chosen and gradient based samples are chosen based on which samples have the greatest gradient, most variation between predicted label and actual label\n",
    "#colsample_bylevel,colsample_bytree,colsample_bynode control the amount of features(columns) can be used at each by object so colsample_bylevel=0.5 means that only 50% of the available features can be used to split at the level decreasing features used can reduce overfitting by forcing the tree to use different features\n",
    "#lambda alpha are L1,L2 regularization terms respectively increasing them increases how conservative the model is\n",
    "#tree_method controls tree construction algorithm exact approx hist\n",
    "#scale_pos_weight controls the balance of positive and negative weights\n",
    "#updater A comma separated string defining the sequence of tree updaters to run, providing a modular way to construct and to modify the trees.\n",
    "#process_type type of boosting process to run we don't need to change for us\n",
    "#grow_policy controls how new nodes are added to the tree\n",
    "#max_leaves controls maxiumum number of nodes to add\n",
    "\n",
    "\n",
    "#Learning Task Parameters\n",
    "\n",
    "#objective controls which objective function we use\n",
    "#base_score controls initial prediction of all instances\n",
    "#eval_metric evaluation metrics for validation data\n",
    "#seed random number seed\n",
    "\n",
    "\n",
    "params = {'objective':'binary:logistic','eval_metric':'rmse','eta':0.3,'gamma':0,'max_depth':6,'min_child_weight':1,'max_delta_step':0,'subsample':1,'sampling_method':'uniform','colsample_bylevel':1,'colsample_bytree':1,'colsample_bynode':1,'alpha':0,'lambda':1,'tree_method':'auto','scale_pos_weight':1,'refresh_leaf':1,'process_type':'default','grow_policy':'depthwise','max_leaves':0,'max_bin':256,}\n",
    "\n",
    "evalList = [(dTrain, 'train'), (dTest, 'eval')]\n",
    "\n",
    "\"\"\"params (Dict[str, Any]) – Booster params.\n",
    "\n",
    "dtrain (DMatrix) – Data to be trained.\n",
    "\n",
    "num_boost_round (int) – Number of boosting iterations.\n",
    "\n",
    "evals (Sequence[Tuple[DMatrix, str]] | None) – List of validation sets for which metrics will evaluated during training. Validation metrics will help us track the performance of the model.\"\"\"\n",
    "\n",
    "bst=xgb.train(params=params, dtrain=dTrain, num_boost_round=100, evals=evalList, verbose_eval=True)\n",
    "\n"
   ],
   "id": "c1d42be88109ae29",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-rmse:0.20476\teval-rmse:0.20980\n",
      "[1]\ttrain-rmse:0.19778\teval-rmse:0.20914\n",
      "[2]\ttrain-rmse:0.19216\teval-rmse:0.20845\n",
      "[3]\ttrain-rmse:0.18922\teval-rmse:0.20837\n",
      "[4]\ttrain-rmse:0.18478\teval-rmse:0.20874\n",
      "[5]\ttrain-rmse:0.18272\teval-rmse:0.20891\n",
      "[6]\ttrain-rmse:0.18021\teval-rmse:0.20947\n",
      "[7]\ttrain-rmse:0.17924\teval-rmse:0.20994\n",
      "[8]\ttrain-rmse:0.17705\teval-rmse:0.20988\n",
      "[9]\ttrain-rmse:0.17611\teval-rmse:0.20981\n",
      "[10]\ttrain-rmse:0.17432\teval-rmse:0.21050\n",
      "[11]\ttrain-rmse:0.17367\teval-rmse:0.21055\n",
      "[12]\ttrain-rmse:0.17323\teval-rmse:0.21039\n",
      "[13]\ttrain-rmse:0.17142\teval-rmse:0.21092\n",
      "[14]\ttrain-rmse:0.16876\teval-rmse:0.21132\n",
      "[15]\ttrain-rmse:0.16845\teval-rmse:0.21162\n",
      "[16]\ttrain-rmse:0.16668\teval-rmse:0.21120\n",
      "[17]\ttrain-rmse:0.16504\teval-rmse:0.21119\n",
      "[18]\ttrain-rmse:0.16345\teval-rmse:0.21038\n",
      "[19]\ttrain-rmse:0.16156\teval-rmse:0.21035\n",
      "[20]\ttrain-rmse:0.16119\teval-rmse:0.21038\n",
      "[21]\ttrain-rmse:0.16018\teval-rmse:0.21078\n",
      "[22]\ttrain-rmse:0.15938\teval-rmse:0.21089\n",
      "[23]\ttrain-rmse:0.15837\teval-rmse:0.21101\n",
      "[24]\ttrain-rmse:0.15753\teval-rmse:0.21157\n",
      "[25]\ttrain-rmse:0.15543\teval-rmse:0.21174\n",
      "[26]\ttrain-rmse:0.15350\teval-rmse:0.21179\n",
      "[27]\ttrain-rmse:0.14990\teval-rmse:0.21121\n",
      "[28]\ttrain-rmse:0.14868\teval-rmse:0.21162\n",
      "[29]\ttrain-rmse:0.14817\teval-rmse:0.21162\n",
      "[30]\ttrain-rmse:0.14391\teval-rmse:0.21183\n",
      "[31]\ttrain-rmse:0.14261\teval-rmse:0.21188\n",
      "[32]\ttrain-rmse:0.14201\teval-rmse:0.21199\n",
      "[33]\ttrain-rmse:0.14097\teval-rmse:0.21230\n",
      "[34]\ttrain-rmse:0.13858\teval-rmse:0.21236\n",
      "[35]\ttrain-rmse:0.13787\teval-rmse:0.21276\n",
      "[36]\ttrain-rmse:0.13698\teval-rmse:0.21282\n",
      "[37]\ttrain-rmse:0.13604\teval-rmse:0.21318\n",
      "[38]\ttrain-rmse:0.13525\teval-rmse:0.21292\n",
      "[39]\ttrain-rmse:0.13501\teval-rmse:0.21296\n",
      "[40]\ttrain-rmse:0.13347\teval-rmse:0.21345\n",
      "[41]\ttrain-rmse:0.13223\teval-rmse:0.21291\n",
      "[42]\ttrain-rmse:0.12989\teval-rmse:0.21361\n",
      "[43]\ttrain-rmse:0.12900\teval-rmse:0.21344\n",
      "[44]\ttrain-rmse:0.12849\teval-rmse:0.21323\n",
      "[45]\ttrain-rmse:0.12810\teval-rmse:0.21330\n",
      "[46]\ttrain-rmse:0.12757\teval-rmse:0.21302\n",
      "[47]\ttrain-rmse:0.12569\teval-rmse:0.21399\n",
      "[48]\ttrain-rmse:0.12498\teval-rmse:0.21350\n",
      "[49]\ttrain-rmse:0.12464\teval-rmse:0.21333\n",
      "[50]\ttrain-rmse:0.12324\teval-rmse:0.21335\n",
      "[51]\ttrain-rmse:0.12223\teval-rmse:0.21342\n",
      "[52]\ttrain-rmse:0.11972\teval-rmse:0.21363\n",
      "[53]\ttrain-rmse:0.11844\teval-rmse:0.21398\n",
      "[54]\ttrain-rmse:0.11762\teval-rmse:0.21405\n",
      "[55]\ttrain-rmse:0.11537\teval-rmse:0.21481\n",
      "[56]\ttrain-rmse:0.11439\teval-rmse:0.21527\n",
      "[57]\ttrain-rmse:0.11407\teval-rmse:0.21521\n",
      "[58]\ttrain-rmse:0.11309\teval-rmse:0.21549\n",
      "[59]\ttrain-rmse:0.11177\teval-rmse:0.21588\n",
      "[60]\ttrain-rmse:0.11087\teval-rmse:0.21578\n",
      "[61]\ttrain-rmse:0.10899\teval-rmse:0.21627\n",
      "[62]\ttrain-rmse:0.10756\teval-rmse:0.21615\n",
      "[63]\ttrain-rmse:0.10692\teval-rmse:0.21603\n",
      "[64]\ttrain-rmse:0.10502\teval-rmse:0.21606\n",
      "[65]\ttrain-rmse:0.10494\teval-rmse:0.21619\n",
      "[66]\ttrain-rmse:0.10485\teval-rmse:0.21636\n",
      "[67]\ttrain-rmse:0.10432\teval-rmse:0.21656\n",
      "[68]\ttrain-rmse:0.10348\teval-rmse:0.21641\n",
      "[69]\ttrain-rmse:0.10217\teval-rmse:0.21672\n",
      "[70]\ttrain-rmse:0.10208\teval-rmse:0.21668\n",
      "[71]\ttrain-rmse:0.10050\teval-rmse:0.21698\n",
      "[72]\ttrain-rmse:0.09837\teval-rmse:0.21724\n",
      "[73]\ttrain-rmse:0.09791\teval-rmse:0.21748\n",
      "[74]\ttrain-rmse:0.09759\teval-rmse:0.21752\n",
      "[75]\ttrain-rmse:0.09541\teval-rmse:0.21782\n",
      "[76]\ttrain-rmse:0.09472\teval-rmse:0.21766\n",
      "[77]\ttrain-rmse:0.09367\teval-rmse:0.21805\n",
      "[78]\ttrain-rmse:0.09214\teval-rmse:0.21836\n",
      "[79]\ttrain-rmse:0.09044\teval-rmse:0.21843\n",
      "[80]\ttrain-rmse:0.08983\teval-rmse:0.21884\n",
      "[81]\ttrain-rmse:0.08891\teval-rmse:0.21919\n",
      "[82]\ttrain-rmse:0.08709\teval-rmse:0.21913\n",
      "[83]\ttrain-rmse:0.08621\teval-rmse:0.21906\n",
      "[84]\ttrain-rmse:0.08613\teval-rmse:0.21911\n",
      "[85]\ttrain-rmse:0.08555\teval-rmse:0.21914\n",
      "[86]\ttrain-rmse:0.08551\teval-rmse:0.21912\n",
      "[87]\ttrain-rmse:0.08537\teval-rmse:0.21925\n",
      "[88]\ttrain-rmse:0.08472\teval-rmse:0.21926\n",
      "[89]\ttrain-rmse:0.08306\teval-rmse:0.21908\n",
      "[90]\ttrain-rmse:0.08252\teval-rmse:0.21911\n",
      "[91]\ttrain-rmse:0.08097\teval-rmse:0.21864\n",
      "[92]\ttrain-rmse:0.07943\teval-rmse:0.21897\n",
      "[93]\ttrain-rmse:0.07878\teval-rmse:0.21922\n",
      "[94]\ttrain-rmse:0.07827\teval-rmse:0.21931\n",
      "[95]\ttrain-rmse:0.07749\teval-rmse:0.21908\n",
      "[96]\ttrain-rmse:0.07682\teval-rmse:0.21926\n",
      "[97]\ttrain-rmse:0.07598\teval-rmse:0.21935\n",
      "[98]\ttrain-rmse:0.07569\teval-rmse:0.21948\n",
      "[99]\ttrain-rmse:0.07512\teval-rmse:0.21939\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "From this it is very obvious we are overfitting, as we go through the epochs the evaluation error keeps going up while our train error get very low we are overfitting on the train data. Additionally rmse does not seem like a good evalulation metric for our data set as the stroke dataset is very imbalanced meaning that model will ignore strokes because minimizing RMSE doesn’t care about sensitivity/recall.",
   "id": "1155d4ee7e42860b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:24:50.362284Z",
     "start_time": "2025-11-28T18:24:50.004749Z"
    }
   },
   "cell_type": "code",
   "source": [
    "params['eval_metric']='aucpr'\n",
    "#auc is a good metric for classification which is the problem we are trying to solve\n",
    "bst1=xgb.train(params=params, dtrain=dTrain, num_boost_round=100, evals=evalList, verbose_eval=True)"
   ],
   "id": "b102a2ec270322f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-aucpr:0.28084\teval-aucpr:0.17376\n",
      "[1]\ttrain-aucpr:0.39747\teval-aucpr:0.17909\n",
      "[2]\ttrain-aucpr:0.46409\teval-aucpr:0.18289\n",
      "[3]\ttrain-aucpr:0.48939\teval-aucpr:0.18365\n",
      "[4]\ttrain-aucpr:0.53827\teval-aucpr:0.17930\n",
      "[5]\ttrain-aucpr:0.55980\teval-aucpr:0.18256\n",
      "[6]\ttrain-aucpr:0.58785\teval-aucpr:0.17698\n",
      "[7]\ttrain-aucpr:0.59784\teval-aucpr:0.17397\n",
      "[8]\ttrain-aucpr:0.62100\teval-aucpr:0.17546\n",
      "[9]\ttrain-aucpr:0.63257\teval-aucpr:0.17670\n",
      "[10]\ttrain-aucpr:0.65740\teval-aucpr:0.17183\n",
      "[11]\ttrain-aucpr:0.66294\teval-aucpr:0.17152\n",
      "[12]\ttrain-aucpr:0.66645\teval-aucpr:0.17654\n",
      "[13]\ttrain-aucpr:0.67731\teval-aucpr:0.17308\n",
      "[14]\ttrain-aucpr:0.70627\teval-aucpr:0.17261\n",
      "[15]\ttrain-aucpr:0.71046\teval-aucpr:0.15987\n",
      "[16]\ttrain-aucpr:0.72426\teval-aucpr:0.16842\n",
      "[17]\ttrain-aucpr:0.72983\teval-aucpr:0.16993\n",
      "[18]\ttrain-aucpr:0.74200\teval-aucpr:0.18719\n",
      "[19]\ttrain-aucpr:0.75915\teval-aucpr:0.18296\n",
      "[20]\ttrain-aucpr:0.76136\teval-aucpr:0.18417\n",
      "[21]\ttrain-aucpr:0.77017\teval-aucpr:0.18222\n",
      "[22]\ttrain-aucpr:0.77953\teval-aucpr:0.17983\n",
      "[23]\ttrain-aucpr:0.78661\teval-aucpr:0.16696\n",
      "[24]\ttrain-aucpr:0.79002\teval-aucpr:0.16274\n",
      "[25]\ttrain-aucpr:0.81102\teval-aucpr:0.16256\n",
      "[26]\ttrain-aucpr:0.82535\teval-aucpr:0.16030\n",
      "[27]\ttrain-aucpr:0.85656\teval-aucpr:0.16838\n",
      "[28]\ttrain-aucpr:0.86423\teval-aucpr:0.16989\n",
      "[29]\ttrain-aucpr:0.86949\teval-aucpr:0.17031\n",
      "[30]\ttrain-aucpr:0.88946\teval-aucpr:0.16946\n",
      "[31]\ttrain-aucpr:0.89434\teval-aucpr:0.17030\n",
      "[32]\ttrain-aucpr:0.89782\teval-aucpr:0.17021\n",
      "[33]\ttrain-aucpr:0.90218\teval-aucpr:0.17032\n",
      "[34]\ttrain-aucpr:0.91185\teval-aucpr:0.16987\n",
      "[35]\ttrain-aucpr:0.91472\teval-aucpr:0.16855\n",
      "[36]\ttrain-aucpr:0.91911\teval-aucpr:0.16760\n",
      "[37]\ttrain-aucpr:0.92130\teval-aucpr:0.16464\n",
      "[38]\ttrain-aucpr:0.92488\teval-aucpr:0.16531\n",
      "[39]\ttrain-aucpr:0.92468\teval-aucpr:0.16493\n",
      "[40]\ttrain-aucpr:0.93109\teval-aucpr:0.16137\n",
      "[41]\ttrain-aucpr:0.93606\teval-aucpr:0.16436\n",
      "[42]\ttrain-aucpr:0.94355\teval-aucpr:0.16267\n",
      "[43]\ttrain-aucpr:0.94670\teval-aucpr:0.16821\n",
      "[44]\ttrain-aucpr:0.94836\teval-aucpr:0.16900\n",
      "[45]\ttrain-aucpr:0.94998\teval-aucpr:0.17159\n",
      "[46]\ttrain-aucpr:0.95126\teval-aucpr:0.17036\n",
      "[47]\ttrain-aucpr:0.95555\teval-aucpr:0.17281\n",
      "[48]\ttrain-aucpr:0.95788\teval-aucpr:0.17822\n",
      "[49]\ttrain-aucpr:0.95865\teval-aucpr:0.18002\n",
      "[50]\ttrain-aucpr:0.96223\teval-aucpr:0.18002\n",
      "[51]\ttrain-aucpr:0.96552\teval-aucpr:0.17463\n",
      "[52]\ttrain-aucpr:0.97158\teval-aucpr:0.17089\n",
      "[53]\ttrain-aucpr:0.97463\teval-aucpr:0.16977\n",
      "[54]\ttrain-aucpr:0.97576\teval-aucpr:0.16572\n",
      "[55]\ttrain-aucpr:0.97785\teval-aucpr:0.16320\n",
      "[56]\ttrain-aucpr:0.97926\teval-aucpr:0.15843\n",
      "[57]\ttrain-aucpr:0.97979\teval-aucpr:0.15858\n",
      "[58]\ttrain-aucpr:0.98142\teval-aucpr:0.15695\n",
      "[59]\ttrain-aucpr:0.98353\teval-aucpr:0.15677\n",
      "[60]\ttrain-aucpr:0.98439\teval-aucpr:0.15840\n",
      "[61]\ttrain-aucpr:0.98543\teval-aucpr:0.15709\n",
      "[62]\ttrain-aucpr:0.98775\teval-aucpr:0.15565\n",
      "[63]\ttrain-aucpr:0.98861\teval-aucpr:0.15677\n",
      "[64]\ttrain-aucpr:0.99044\teval-aucpr:0.15937\n",
      "[65]\ttrain-aucpr:0.99058\teval-aucpr:0.15871\n",
      "[66]\ttrain-aucpr:0.99049\teval-aucpr:0.15905\n",
      "[67]\ttrain-aucpr:0.99079\teval-aucpr:0.15858\n",
      "[68]\ttrain-aucpr:0.99146\teval-aucpr:0.15890\n",
      "[69]\ttrain-aucpr:0.99220\teval-aucpr:0.15646\n",
      "[70]\ttrain-aucpr:0.99232\teval-aucpr:0.15718\n",
      "[71]\ttrain-aucpr:0.99281\teval-aucpr:0.15721\n",
      "[72]\ttrain-aucpr:0.99362\teval-aucpr:0.15783\n",
      "[73]\ttrain-aucpr:0.99365\teval-aucpr:0.15282\n",
      "[74]\ttrain-aucpr:0.99372\teval-aucpr:0.15299\n",
      "[75]\ttrain-aucpr:0.99493\teval-aucpr:0.15326\n",
      "[76]\ttrain-aucpr:0.99507\teval-aucpr:0.15351\n",
      "[77]\ttrain-aucpr:0.99571\teval-aucpr:0.15192\n",
      "[78]\ttrain-aucpr:0.99629\teval-aucpr:0.15222\n",
      "[79]\ttrain-aucpr:0.99700\teval-aucpr:0.15162\n",
      "[80]\ttrain-aucpr:0.99708\teval-aucpr:0.14962\n",
      "[81]\ttrain-aucpr:0.99746\teval-aucpr:0.14788\n",
      "[82]\ttrain-aucpr:0.99718\teval-aucpr:0.15057\n",
      "[83]\ttrain-aucpr:0.99727\teval-aucpr:0.14975\n",
      "[84]\ttrain-aucpr:0.99734\teval-aucpr:0.14949\n",
      "[85]\ttrain-aucpr:0.99792\teval-aucpr:0.15000\n",
      "[86]\ttrain-aucpr:0.99792\teval-aucpr:0.14911\n",
      "[87]\ttrain-aucpr:0.99795\teval-aucpr:0.14916\n",
      "[88]\ttrain-aucpr:0.99835\teval-aucpr:0.14953\n",
      "[89]\ttrain-aucpr:0.99842\teval-aucpr:0.15090\n",
      "[90]\ttrain-aucpr:0.99844\teval-aucpr:0.15211\n",
      "[91]\ttrain-aucpr:0.99877\teval-aucpr:0.15414\n",
      "[92]\ttrain-aucpr:0.99930\teval-aucpr:0.15383\n",
      "[93]\ttrain-aucpr:0.99947\teval-aucpr:0.15347\n",
      "[94]\ttrain-aucpr:0.99958\teval-aucpr:0.15365\n",
      "[95]\ttrain-aucpr:0.99944\teval-aucpr:0.15228\n",
      "[96]\ttrain-aucpr:0.99947\teval-aucpr:0.15327\n",
      "[97]\ttrain-aucpr:0.99961\teval-aucpr:0.15234\n",
      "[98]\ttrain-aucpr:0.99966\teval-aucpr:0.15167\n",
      "[99]\ttrain-aucpr:0.99966\teval-aucpr:0.15183\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Changing the eval metric to AUC gives us a better view of the issue and our overfitting problem. To solve there are a couple things we can consider\n",
    "\n",
    "early stopping when the evalulation auc does not go significantly up after a certain number of runs to stop overfitting early\n",
    "\n",
    "changing our other parameters to avoid overfitting in the\n",
    "\n",
    "While xgboost library does not have a grid search feature to help optimize parameter finding, sklearn does and xgboost has a sklearn wrapper to allow us to use both xgboost and the sklearn functions"
   ],
   "id": "3404252732367b7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:24:50.371552Z",
     "start_time": "2025-11-28T18:24:50.367843Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\"\"\"scale_pos_weights Control the balance of positive and negative weights, useful for unbalanced classes. A typical value to consider: sum(negative instances) / sum(positive instances)\"\"\"\n",
    "neg = (strokeDataLabels == 0).sum()\n",
    "pos = (strokeDataLabels == 1).sum()\n",
    "\n",
    "scale_pos_weight = neg / pos\n",
    "print(scale_pos_weight)"
   ],
   "id": "68e01c993dacaa96",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.522088353413654\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We have alot of parameters to check so we want to get rid of some that we can\n",
    "\n",
    "For the most part we can set some of these and not use grid search for them such as subsample, to help overfitting we can set it to 0.7 along with colsample_bytree we can just set that to 0.7 and min_child_weight can be set to 3\n",
    "\n",
    "Since our dataset is relatively small compared to other data sets we can use an exact tree method rather than a hist allowing us to get rid of some parameters like grow_policy and max_leaves"
   ],
   "id": "ba3b500035afe04e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:49:43.736772Z",
     "start_time": "2025-11-28T18:44:25.046414Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "bstSklearnWrapper=xgb.XGBClassifier(n_estimators=100, max_depth=6,tree_method='exact',learning_rate=0.3,objective=\"binary:logistic\", booster='gbtree',gamma=0,min_child_weight=3,max_delta_step=0,subsample=0.7,colsample_bytree=0.7,colsample_bylevel=1,colsample_bynode=1,reg_alpha=0,reg_lambda=1,scale_pos_weight=scale_pos_weight,random_state=42,eval_metric='aucpr')\n",
    "paramGrid={'max_depth':[3,5,8],'learning_rate':[0.01,0.1,0.3],'gamma':[0,1,3],'max_delta_step':[0,1],'reg_alpha':[0,1],'reg_lambda':[1,3]}\n",
    "grid=GridSearchCV(estimator=bstSklearnWrapper,param_grid=paramGrid,cv=5,scoring='average_precision',verbose=1)\n",
    "grid.fit(trainFeatures, trainLabels)\n"
   ],
   "id": "4a74b8ff56414148",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 216 candidates, totalling 1080 fits\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "d4eef98465a622c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:51:39.928045Z",
     "start_time": "2025-11-28T18:51:39.924353Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bestModel=grid.best_estimator_\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)\n"
   ],
   "id": "54dc4d29874d421e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gamma': 0, 'learning_rate': 0.3, 'max_delta_step': 1, 'max_depth': 3, 'reg_alpha': 1, 'reg_lambda': 3}\n",
      "0.22076687858439445\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-28T18:51:41.992917Z",
     "start_time": "2025-11-28T18:51:41.974932Z"
    }
   },
   "cell_type": "code",
   "source": [
    "yTestProba = bestModel.predict_proba(testFeatures)[:, 1]\n",
    "yTestPred  = (yTestProba >= 0.5).astype(int)\n",
    "\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, classification_report\n",
    "\n",
    "print(\"Test PR-AUC:\", average_precision_score(testLabels, yTestProba))\n",
    "print(\"Test ROC-AUC:\", roc_auc_score(testLabels, yTestProba))\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(testLabels, yTestPred, digits=4))"
   ],
   "id": "2fa595ca823fb201",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test PR-AUC: 0.2008742209888832\n",
      "Test ROC-AUC: 0.80679012345679\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9749    0.8395    0.9022       972\n",
      "           1     0.1568    0.5800    0.2468        50\n",
      "\n",
      "    accuracy                         0.8268      1022\n",
      "   macro avg     0.5658    0.7098    0.5745      1022\n",
      "weighted avg     0.9349    0.8268    0.8701      1022\n",
      "\n"
     ]
    }
   ],
   "execution_count": 25
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
